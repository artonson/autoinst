{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "%matplotlib inline \n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../..\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "%load_ext autoreload\n",
    "from dataset.nuscenes_dataset import nuScenesOdometryDataset, nuScenesDatasetConfig\n",
    "from dataset.filters.filter_list import FilterList\n",
    "from dataset.filters.kitti_gt_mo_filter import KittiGTMovingObjectFilter\n",
    "from dataset.filters.range_filter import RangeFilter\n",
    "from dataset.filters.apply_pose import ApplyPose\n",
    "\n",
    "from hidden_points_removal import hidden_point_removal_o3d, hidden_point_removal_biasutti\n",
    "from point_cloud_utils import transform_pcd, filter_points_from_dict, get_pcd, point_to_label, change_point_indices\n",
    "from point_to_pixels import point_to_pixel\n",
    "from visualization_utils import unite_pcd_and_img, color_pcd_with_labels, visualize_associations_in_img\n",
    "from merge_pointclouds import build_associations, apply_associations_to_dict, merge_label_predictions, merge_pointclouds, build_associations_across_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset depending on kitti sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/Users/laurenzheidrich/Downloads/nuScenes'\n",
    "SEQUENCE_NUM = 1\n",
    "\n",
    "config_filtered = nuScenesDatasetConfig(\n",
    "    cache=True,\n",
    "    dataset_path=DATASET_PATH,\n",
    "    filters=FilterList(\n",
    "        [\n",
    "            KittiGTMovingObjectFilter(\n",
    "                os.path.join(\n",
    "                    DATASET_PATH,\n",
    "                    \"sequences\",\n",
    "                    \"%.2d\" % SEQUENCE_NUM,\n",
    "                    \"labels\",\n",
    "                )\n",
    "            ),\n",
    "            RangeFilter(2.5, 120),\n",
    "            ApplyPose(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "dataset = nuScenesOdometryDataset(config_filtered, SEQUENCE_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the point cloud and the left and right image of the stereo camera. If labels for those images are available they can be read in, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_index = 5\n",
    "front_cam = \"CAM_FRONT\"\n",
    "left_cam = \"CAM_FRONT_LEFT\"\n",
    "right_cam = \"CAM_FRONT_RIGHT\"\n",
    "\n",
    "pcd_o3d = get_pcd(dataset.get_point_cloud(points_index))\n",
    "pcd = np.asarray(pcd_o3d.points)\n",
    "\n",
    "front_image_PIL = dataset.get_image(front_cam, points_index)\n",
    "front_image = cv2.cvtColor(np.array(front_image_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "left_image_PIL = dataset.get_image(left_cam, points_index)\n",
    "left_image = cv2.cvtColor(np.array(left_image_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "right_image_PIL = dataset.get_image(right_cam, points_index)\n",
    "right_image = cv2.cvtColor(np.array(right_image_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Only do this, if there is a SAM label present for respective point_index\n",
    "\n",
    "front_label_PIL = dataset.get_sam_label(front_cam, points_index)\n",
    "front_label = cv2.cvtColor(np.array(front_label_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "left_label_PIL = dataset.get_sam_label(left_cam, points_index)\n",
    "left_label = cv2.cvtColor(np.array(left_label_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "right_label_PIL = dataset.get_sam_label(right_cam, points_index)\n",
    "right_label = cv2.cvtColor(np.array(right_label_PIL), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the transformation matrixes and intriniscs for the two cameras and transform the point cloud accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_lidar2leftcam, K_leftcam = dataset.get_calibration_matrices(left_cam)\n",
    "T_lidar2frontcam, K_frontcam = dataset.get_calibration_matrices(front_cam)\n",
    "T_lidar2rightcam, K_rightcam = dataset.get_calibration_matrices(right_cam)\n",
    "\n",
    "pcd_leftcamframe = transform_pcd(pcd, T_lidar2leftcam)\n",
    "pcd_frontcamframe = transform_pcd(pcd, T_lidar2frontcam)\n",
    "pcd_rightcamframe = transform_pcd(pcd, T_lidar2rightcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform hidden point removal on the two camframes, since different points are hidden from different perspectives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpr_mode = \"o3d\" # \"o3d\" or \"biscutti\"\n",
    "\n",
    "if hpr_mode == \"o3d\":\n",
    "    #Camera set to [0,0,0] because we are already in camera frame and camera position is origin\n",
    "    hpr_mask_leftcam = hidden_point_removal_o3d(pcd_leftcamframe, camera=[0,0,0], radius_factor=400) \n",
    "    hpr_mask_frontcam = hidden_point_removal_o3d(pcd_frontcamframe, camera=[0,0,0], radius_factor=400) \n",
    "    hpr_mask_rightcam = hidden_point_removal_o3d(pcd_rightcamframe, camera=[0,0,0], radius_factor=400) \n",
    "elif hpr_mode == \"biasutti\":\n",
    "    hpr_mask_leftcam = hidden_point_removal_biasutti(pcd_leftcamframe, n_neighbours=64)\n",
    "    hpr_mask_frontcam = hidden_point_removal_biasutti(pcd_frontcamframe, n_neighbours=64)\n",
    "    hpr_mask_rightcam = hidden_point_removal_biasutti(pcd_rightcamframe, n_neighbours=64)\n",
    "\n",
    "pcd_leftcamframe_hpr = pcd_leftcamframe[hpr_mask_leftcam]\n",
    "pcd_frontcamframe_hpr = pcd_frontcamframe[hpr_mask_frontcam]\n",
    "pcd_rightcamframe_hpr = pcd_rightcamframe[hpr_mask_rightcam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build up point-to-pixel correspondences from the pointclouds to the respective image frames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_to_pixel_dict_leftcam = point_to_pixel(pcd_leftcamframe, K_leftcam, left_image.shape[0], left_image.shape[1])\n",
    "point_to_pixel_dict_frontcam = point_to_pixel(pcd_frontcamframe, K_frontcam, front_image.shape[0], front_image.shape[1])\n",
    "point_to_pixel_dict_rightcam = point_to_pixel(pcd_rightcamframe, K_rightcam, right_image.shape[0], right_image.shape[1])\n",
    "\n",
    "point_to_pixel_dict_hpr_leftcam = point_to_pixel(pcd_leftcamframe_hpr, K_leftcam, left_image.shape[0], left_image.shape[1])\n",
    "point_to_pixel_dict_hpr_frontcam = point_to_pixel(pcd_frontcamframe_hpr, K_frontcam, front_image.shape[0], front_image.shape[1])\n",
    "point_to_pixel_dict_hpr_rightcam = point_to_pixel(pcd_rightcamframe_hpr, K_rightcam, right_image.shape[0], right_image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of points in initial point cloud: \", pcd.shape[0])\n",
    "print(\"Number of points after Hidden Point Removal from perspective of left camera: \", pcd_leftcamframe_hpr.shape[0])\n",
    "print(\"Number of points after Hidden Point Removal from perspective of front camera: \", pcd_frontcamframe_hpr.shape[0])\n",
    "print(\"Number of points after Hidden Point Removal from perspective of right camera: \", pcd_rightcamframe_hpr.shape[0])\n",
    "print(\"Number of points projected on left image without Hidden Point Removal: \", len(point_to_pixel_dict_leftcam))\n",
    "print(\"Number of points projected on front image without Hidden Point Removal: \", len(point_to_pixel_dict_frontcam))\n",
    "print(\"Number of points projected on right image without Hidden Point Removal: \", len(point_to_pixel_dict_rightcam))\n",
    "print(\"Number of points projected on left image with Hidden Point Removal: \", len(point_to_pixel_dict_hpr_leftcam))\n",
    "print(\"Number of points projected on front image with Hidden Point Removal: \", len(point_to_pixel_dict_hpr_frontcam))\n",
    "print(\"Number of points projected on right image with Hidden Point Removal: \", len(point_to_pixel_dict_hpr_rightcam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we unite the point clouds with the images and color the point clouds according to the available labels or the depth of the points onto the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring either with depth or label_map, depending on wether label is available\n",
    "left_img_overlay = unite_pcd_and_img(point_to_pixel_dict_leftcam, left_image, left_label, coloring=\"label_map\", radius=5)\n",
    "front_img_overlay = unite_pcd_and_img(point_to_pixel_dict_frontcam, front_image, front_label, coloring=\"label_map\", radius=5)\n",
    "right_img_overlay = unite_pcd_and_img(point_to_pixel_dict_rightcam, right_image, right_label, coloring=\"label_map\", radius=5)\n",
    "\n",
    "left_img_overlay_hpr = unite_pcd_and_img(point_to_pixel_dict_hpr_leftcam, left_image, left_label, coloring=\"label_map\", radius=5)\n",
    "front_img_overlay_hpr = unite_pcd_and_img(point_to_pixel_dict_hpr_frontcam, front_image, front_label, coloring=\"label_map\", radius=5)\n",
    "right_img_overlay_hpr = unite_pcd_and_img(point_to_pixel_dict_hpr_rightcam, right_image, right_label, coloring=\"label_map\", radius=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3)\n",
    "\n",
    "images = [left_image, front_image, right_image, left_label, front_label, right_label, left_img_overlay, front_img_overlay, right_img_overlay, left_img_overlay_hpr, front_img_overlay_hpr, right_img_overlay_hpr]\n",
    "\n",
    "titles = [\"Left Image\", \"Front Image\", \"Right Image\", \"Left Label\", \"Front Label\", \"Right Label\", \"Left Image Overlay\", \"Front Image Overlay\", \"Right Image Overlay\", \"Left Image Overlay HPR\", \"Front Image Overlay HPR\", \"Right Image Overlay HPR\"]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title(titles[i], fontsize=6)\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "fig.set_dpi(300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we filter the point clouds according to the points that are visible within the image frames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_leftcamframe_fov = filter_points_from_dict(pcd, point_to_pixel_dict_leftcam)\n",
    "pcd_frontcamframe_fov = filter_points_from_dict(pcd, point_to_pixel_dict_frontcam)\n",
    "pcd_rightcamframe_fov = filter_points_from_dict(pcd, point_to_pixel_dict_rightcam)\n",
    "\n",
    "pcd_leftcamframe_fov_hpr = filter_points_from_dict(pcd_leftcamframe_hpr, point_to_pixel_dict_hpr_leftcam)\n",
    "pcd_frontcamframe_fov_hpr = filter_points_from_dict(pcd_frontcamframe_hpr, point_to_pixel_dict_hpr_frontcam)\n",
    "pcd_rightcamframe_fov_hpr = filter_points_from_dict(pcd_rightcamframe_hpr, point_to_pixel_dict_hpr_rightcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of those points, we build up correspondences that map the point index to label color!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_to_label_dict_leftcam = point_to_label(point_to_pixel_dict_leftcam, left_label)\n",
    "point_to_label_dict_frontcam = point_to_label(point_to_pixel_dict_frontcam, front_label)\n",
    "point_to_label_dict_rightcam = point_to_label(point_to_pixel_dict_rightcam, right_label)\n",
    "\n",
    "point_to_label_dict_hpr_leftcam = point_to_label(point_to_pixel_dict_hpr_leftcam, left_label)\n",
    "point_to_label_dict_hpr_frontcam = point_to_label(point_to_pixel_dict_hpr_frontcam, front_label)\n",
    "point_to_label_dict_hpr_rightcam = point_to_label(point_to_pixel_dict_hpr_rightcam, right_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those correspondences, we can now color the point clouds with the colors from the labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_leftcamframe_fov = color_pcd_with_labels(pcd_leftcamframe_fov, point_to_label_dict_leftcam)\n",
    "pcd_frontcamframe_fov = color_pcd_with_labels(pcd_frontcamframe_fov, point_to_label_dict_frontcam)\n",
    "pcd_rightcamframe_fov = color_pcd_with_labels(pcd_rightcamframe_fov, point_to_label_dict_rightcam)\n",
    "\n",
    "pcd_leftcamframe_fov_hpr = color_pcd_with_labels(pcd_leftcamframe_fov_hpr, point_to_label_dict_hpr_leftcam)\n",
    "pcd_frontcamframe_fov_hpr = color_pcd_with_labels(pcd_frontcamframe_fov_hpr, point_to_label_dict_hpr_frontcam)\n",
    "pcd_rightcamframe_fov_hpr = color_pcd_with_labels(pcd_rightcamframe_fov_hpr, point_to_label_dict_hpr_rightcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.io.write_point_cloud(\"filtered.pcd\", pcd_rightcamframe_fov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
