{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "%matplotlib inline \n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(\"../..\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "%load_ext autoreload\n",
    "from dataset.kitti_odometry_dataset import KittiOdometryDataset, KittiOdometryDatasetConfig\n",
    "from dataset.filters.filter_list import FilterList\n",
    "from dataset.filters.kitti_gt_mo_filter import KittiGTMovingObjectFilter\n",
    "from dataset.filters.range_filter import RangeFilter\n",
    "from dataset.filters.apply_pose import ApplyPose\n",
    "\n",
    "from hidden_points_removal import hidden_point_removal_o3d, hidden_point_removal_biasutti\n",
    "from point_cloud_utils import transform_pcd, filter_points_from_dict, get_pcd, point_to_label, change_point_indices\n",
    "from point_to_pixels import point_to_pixel\n",
    "from visualization_utils import unite_pcd_and_img, color_pcd_with_labels, visualize_associations_in_img\n",
    "from merge_pointclouds import build_associations, apply_associations_to_dict, merge_label_predictions, merge_pointclouds, build_associations_across_timesteps\n",
    "from merged_sequences import merged_sequence\n",
    "from image_utils import masks_to_image, masks_to_colored_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset depending on kitti sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/Users/laurenzheidrich/Downloads/','fused_dataset')\n",
    "SEQUENCE_NUM = 7\n",
    "\n",
    "config_filtered = KittiOdometryDatasetConfig(\n",
    "    cache=True,\n",
    "    dataset_path=DATASET_PATH,\n",
    "    correct_scan_calibration=True,\n",
    "    filters=FilterList(\n",
    "        [\n",
    "            KittiGTMovingObjectFilter(\n",
    "                os.path.join(\n",
    "                    DATASET_PATH,\n",
    "                    \"sequences\",\n",
    "                    \"%.2d\" % SEQUENCE_NUM,\n",
    "                    \"labels\",\n",
    "                )\n",
    "            ),\n",
    "            RangeFilter(2.5, 120),\n",
    "            ApplyPose(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "dataset = KittiOdometryDataset(config_filtered, SEQUENCE_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the point cloud and the left and right image of the stereo camera. If labels for those images are available they can be read in, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_index = 100\n",
    "left_cam = \"cam2\"\n",
    "right_cam = \"cam3\"\n",
    "\n",
    "pcd_o3d = get_pcd(dataset.get_point_cloud(points_index))\n",
    "pcd = np.asarray(pcd_o3d.points)\n",
    "\n",
    "left_image_PIL = dataset.get_image(left_cam, points_index)\n",
    "left_image = cv2.cvtColor(np.array(left_image_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "right_image_PIL = dataset.get_image(right_cam, points_index)\n",
    "right_image = cv2.cvtColor(np.array(right_image_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Only do this, if there is a SAM label present for respective point_index\n",
    "\n",
    "left_label_PIL = dataset.get_sam_label(left_cam, points_index)\n",
    "left_label = cv2.cvtColor(np.array(left_label_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "right_label_PIL = dataset.get_sam_label(right_cam, points_index)\n",
    "right_label = cv2.cvtColor(np.array(right_label_PIL), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "left_masks = dataset.get_sam_mask(left_cam, points_index)\n",
    "left_labels_from_mask = masks_to_image(left_masks)\n",
    "colored_masks_left = masks_to_colored_image(left_masks).astype(int)\n",
    "\n",
    "right_masks = dataset.get_sam_mask(right_cam, points_index)\n",
    "right_labels_from_mask = masks_to_image(right_masks)\n",
    "colored_masks_right = masks_to_colored_image(right_masks).astype(int)\n",
    "\n",
    "#Otherwise use this:\n",
    "#right_label = None\n",
    "#left_label = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the transformation matrixes and intriniscs for the two cameras and transform the point cloud accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_lidar2leftcam, K_leftcam = dataset.get_calibration_matrices(left_cam)\n",
    "T_lidar2rightcam, K_rightcam = dataset.get_calibration_matrices(right_cam)\n",
    "\n",
    "pcd_leftcamframe = transform_pcd(pcd, T_lidar2leftcam)\n",
    "pcd_rightcamframe = transform_pcd(pcd, T_lidar2rightcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform hidden point removal on the two camframes, since different points are hidden from different perspectives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpr_mode = \"o3d\" # \"o3d\" or \"biscutti\"\n",
    "\n",
    "if hpr_mode == \"o3d\":\n",
    "    #Camera set to [0,0,0] because we are already in camera frame and camera position is origin\n",
    "    hpr_mask_leftcam = hidden_point_removal_o3d(pcd_leftcamframe, camera=[0,0,0], radius_factor=400) \n",
    "    hpr_mask_rightcam = hidden_point_removal_o3d(pcd_rightcamframe, camera=[0,0,0], radius_factor=400) \n",
    "elif hpr_mode == \"biasutti\":\n",
    "    hpr_mask_leftcam = hidden_point_removal_biasutti(pcd_leftcamframe, n_neighbours=64)\n",
    "    hpr_mask_rightcam = hidden_point_removal_biasutti(pcd_rightcamframe, n_neighbours=64)\n",
    "\n",
    "pcd_leftcamframe_hpr = pcd_leftcamframe[hpr_mask_leftcam]\n",
    "pcd_rightcamframe_hpr = pcd_rightcamframe[hpr_mask_rightcam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build up point-to-pixel correspondences from the pointclouds to the respective image frames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_to_pixel_dict_leftcam = point_to_pixel(pcd_leftcamframe, K_leftcam, left_image.shape[0], left_image.shape[1])\n",
    "point_to_pixel_dict_rightcam = point_to_pixel(pcd_rightcamframe, K_rightcam, right_image.shape[0], right_image.shape[1])\n",
    "\n",
    "point_to_pixel_dict_hpr_leftcam = point_to_pixel(pcd_leftcamframe_hpr, K_leftcam, left_image.shape[0], left_image.shape[1])\n",
    "point_to_pixel_dict_hpr_rightcam = point_to_pixel(pcd_rightcamframe_hpr, K_rightcam, right_image.shape[0], right_image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of points in initial point cloud: \", pcd.shape[0])\n",
    "print(\"Number of points after Hidden Point Removal from perspective of left camera: \", pcd_leftcamframe_hpr.shape[0])\n",
    "print(\"Number of points after Hidden Point Removal from perspective of right camera: \", pcd_rightcamframe_hpr.shape[0])\n",
    "print(\"Number of points projected on left image without Hidden Point Removal: \", len(point_to_pixel_dict_leftcam))\n",
    "print(\"Number of points projected on right image without Hidden Point Removal: \", len(point_to_pixel_dict_rightcam))\n",
    "print(\"Number of points projected on left image with Hidden Point Removal: \", len(point_to_pixel_dict_hpr_leftcam))\n",
    "print(\"Number of points projected on right image with Hidden Point Removal: \", len(point_to_pixel_dict_hpr_rightcam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we unite the point clouds with the images and color the point clouds according to the available labels or the depth of the points onto the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring either with depth or label_map, depending on wether label is available\n",
    "left_img_overlay = unite_pcd_and_img(point_to_pixel_dict_leftcam, left_image, left_labels_from_mask, coloring=\"label_map\", is_instance=True)\n",
    "right_img_overlay = unite_pcd_and_img(point_to_pixel_dict_hpr_rightcam, right_image, right_labels_from_mask, coloring=\"label_map\", is_instance=True)\n",
    "\n",
    "left_img_overlay_hpr = unite_pcd_and_img(point_to_pixel_dict_hpr_leftcam, left_image, left_labels_from_mask, coloring=\"label_map\", is_instance=True)\n",
    "right_img_overlay_hpr = unite_pcd_and_img(point_to_pixel_dict_hpr_rightcam, right_image, right_labels_from_mask, coloring=\"label_map\", is_instance=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The color values are random!!! ###\n",
    "\n",
    "fig, axes = plt.subplots(4, 2)\n",
    "\n",
    "images = [left_image, right_image, left_label, right_label, left_img_overlay, right_img_overlay, left_img_overlay_hpr, right_img_overlay_hpr]\n",
    "\n",
    "titles = [\"Left Image\", \"Right Image\", \"Left Label\", \"Right Label\", \"Left Image Overlay\", \"Right Image Overlay\", \"Left Image Overlay HPR\", \"Right Image Overlay HPR\"]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title(titles[i], fontsize=6)\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "fig.set_dpi(300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we filter the point clouds according to the points that are visible within the image frames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_leftcamframe_fov = filter_points_from_dict(pcd, point_to_pixel_dict_leftcam)\n",
    "pcd_rightcamframe_fov = filter_points_from_dict(pcd, point_to_pixel_dict_rightcam)\n",
    "\n",
    "pcd_leftcamframe_fov_hpr = filter_points_from_dict(pcd_leftcamframe_hpr, point_to_pixel_dict_hpr_leftcam)\n",
    "pcd_rightcamframe_fov_hpr = filter_points_from_dict(pcd_rightcamframe_hpr, point_to_pixel_dict_hpr_rightcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of those points, we build up correspondences that map the point index to label color!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_to_label_dict_leftcam = point_to_label(point_to_pixel_dict_leftcam, left_label)\n",
    "point_to_label_dict_rightcam = point_to_label(point_to_pixel_dict_rightcam, right_label)\n",
    "\n",
    "point_to_label_dict_hpr_leftcam = point_to_label(point_to_pixel_dict_hpr_leftcam, left_label)\n",
    "point_to_label_dict_hpr_rightcam = point_to_label(point_to_pixel_dict_hpr_rightcam, right_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those correspondences, we can now color the point clouds with the colors from the labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_leftcamframe_fov = color_pcd_with_labels(pcd_leftcamframe_fov, point_to_label_dict_leftcam)\n",
    "pcd_rightcamframe_fov = color_pcd_with_labels(pcd_rightcamframe_fov, point_to_label_dict_rightcam)\n",
    "\n",
    "pcd_leftcamframe_fov_hpr = color_pcd_with_labels(pcd_leftcamframe_fov_hpr, point_to_label_dict_hpr_leftcam)\n",
    "pcd_rightcamframe_fov_hpr = color_pcd_with_labels(pcd_rightcamframe_fov_hpr, point_to_label_dict_hpr_rightcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o3d.visualization.draw_geometries([pcd_rightcamframe_fov])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to join the instance label predictions from left and right camera.\n",
    "As a first step, we build associations between the colors of the instance labels from left & right camera!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to do that with the filtered point cloud, we need to change the indices of both point_to_label_dicts, so that they correspond to the\n",
    "# original point cloud indices\n",
    "point_to_label_dict_hpr_leftcam_mapped = change_point_indices(point_to_label_dict_hpr_leftcam, hpr_mask_leftcam)\n",
    "point_to_label_dict_hpr_rightcam_mapped = change_point_indices(point_to_label_dict_hpr_rightcam, hpr_mask_rightcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations_lr = build_associations(point_to_label_dict_hpr_leftcam_mapped, point_to_label_dict_hpr_rightcam_mapped)\n",
    "associations_rl = build_associations(point_to_label_dict_hpr_rightcam_mapped, point_to_label_dict_hpr_leftcam_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those association can now be visualized on the label predictions. Black areas are areas that are not covered by lidar points, and as such no associations are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_lr = visualize_associations_in_img(left_label, associations_lr)\n",
    "new_label_rl = visualize_associations_in_img(right_label, associations_rl)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "\n",
    "images = [left_label, right_label, new_label_rl, new_label_lr]\n",
    "\n",
    "titles = [\"Left Label\", \"Right Label\", \"Right Label with Associations\", \"Left Label with Associations\"]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title(titles[i], fontsize=6)\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "fig.set_dpi(300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to apply the associations to the dict that maps point cloud indices to the instance colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_to_label_dict_hpr_leftcam_associated = apply_associations_to_dict(point_to_label_dict_hpr_leftcam_mapped, associations_lr)\n",
    "point_to_label_dict_hpr_rightcam_associated = apply_associations_to_dict(point_to_label_dict_hpr_rightcam_mapped, associations_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the point_to_label dict that was associated with the other point_to_label dict to get a merged dict. As of now, this is not sensibly working with HPR, because we might be adding points that were previously removed from HPR. In later steps, this would lead to the program crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_labels_lr = merge_label_predictions(point_to_label_dict_hpr_leftcam_associated, point_to_label_dict_hpr_rightcam_mapped, method='iou')\n",
    "merged_labels_rl = merge_label_predictions(point_to_label_dict_hpr_rightcam_associated, point_to_label_dict_hpr_leftcam_mapped, method='iou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this merged dict we can filter the point cloud and color it accordinly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_merged_labels_lr = filter_points_from_dict(pcd, merged_labels_lr)\n",
    "pcd_merged_labels_lr = color_pcd_with_labels(pcd_merged_labels_lr, merged_labels_lr)\n",
    "\n",
    "pcd_merged_labels_rl = filter_points_from_dict(pcd, merged_labels_rl)\n",
    "pcd_merged_labels_rl = color_pcd_with_labels(pcd_merged_labels_rl, merged_labels_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize the merged point cloud with colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o3d.visualization.draw_geometries([pcd_merged_labels_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to merge multiple point clouds from consecutive timesteps. For this we need to propagate the label maps into the next timeframe. To account for limited compute capabilities, we might need to downsample label maps before building the associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 0.25\n",
    "\n",
    "left_label_t0 = left_label\n",
    "left_label_t0_downsampled = cv2.resize(left_label, (int(scale_factor * left_label_t0.shape[1]), int(scale_factor * left_label_t0.shape[0])), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "left_label_t1_PIL = dataset.get_sam_label(left_cam, points_index+1)\n",
    "left_label_t1 = cv2.cvtColor(np.array(left_label_t1_PIL), cv2.COLOR_RGB2BGR)\n",
    "left_label_t1_downsampled = cv2.resize(left_label_t1, (int(scale_factor * left_label_t1.shape[1]), int(scale_factor * left_label_t1.shape[0])), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "association = build_associations_across_timesteps(left_label_t1_downsampled, left_label_t0_downsampled)\n",
    "left_label_t1_associated = visualize_associations_in_img(left_label_t1, association)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1)\n",
    "\n",
    "images = [left_label_t0, left_label_t1_associated, left_label_t1]\n",
    "\n",
    "titles = [\"Left Label Timestep=t\", \"Left Label Timestep=t+1 with associations\", \"Left Label Timestep=t+1\"]\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_title(titles[i], fontsize=6)\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "fig.set_dpi(100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This principle can be applied to multiple timesteps in a row, applied on the point clouds and then merged together. This is implemented in the function merged_sequence and is applied below (This takes around sequence_length * 3 seconds on my MacBook M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 58\n",
    "sequence_length = 10\n",
    "\n",
    "label_history, pcd_merge = merged_sequence(dataset, start_index, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o3d.io.write_point_cloud(\"pcd_merge.pcd\", pcd_merge, write_ascii=False, compressed=False, print_progress=False)\n",
    "#o3d.visualization.draw_geometries([pcd_merge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can visualize the evolution of propagated label maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(label_history[i])\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout(pad=1.0)\n",
    "fig.set_dpi(500)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
