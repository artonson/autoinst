{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "%matplotlib inline \n",
    "\n",
    "src_path = os.path.abspath(\"../..\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "%load_ext autoreload\n",
    "from dataset.kitti_odometry_dataset import KittiOdometryDataset, KittiOdometryDatasetConfig\n",
    "from dataset.filters.filter_list import FilterList\n",
    "from dataset.filters.kitti_gt_mo_filter import KittiGTMovingObjectFilter\n",
    "from dataset.filters.range_filter import RangeFilter\n",
    "from dataset.filters.apply_pose import ApplyPose\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from normalized_cut import normalized_cut\n",
    "from ncuts_utils import ncuts_chunk,kDTree_1NN_feature_reprojection_colors, get_merge_pcds\n",
    "from dataset_utils import * \n",
    "from point_cloud_utils import get_pcd, transform_pcd, kDTree_1NN_feature_reprojection, remove_isolated_points, get_subpcd, get_statistical_inlier_indices, merge_chunks_unite_instances\n",
    "from aggregate_pointcloud import aggregate_pointcloud\n",
    "from visualization_utils import generate_random_colors, color_pcd_by_labels,generate_random_colors_map\n",
    "from sam_label_distace import sam_label_distance\n",
    "from chunk_generation import subsample_positions, chunks_from_pointcloud, indices_per_patch, tarl_features_per_patch, image_based_features_per_patch, dinov2_mean, get_indices_feature_reprojection\n",
    "from metrics.metrics_class import Metrics\n",
    "import shutil\n",
    "lib_path = os.path.expanduser('~') + '/unsup_3d_instances/pipeline/segmentation/utils/voxel_clustering_dependencies/build/'\n",
    "sys.path.insert(0, lib_path+ \"clustering\")\n",
    "felsenzwalb_path = '/home/cedric/UnScene3D_collaboration_fork/lib/utils/cpp_utils/build/lib.linux-x86_64-cpython-39/'\n",
    "import felzenszwalb_cpp\n",
    "import pycluster\n",
    "from scipy.spatial import KDTree\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset depending on kitti sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/media/cedric/Datasets1/semantic_kitti/')\n",
    "end_inds = {0:4541,1:1100,2:4661,3:800,4:271,5:2761,6:1101,7:1100,8:4071,9:1591,10:1201}\n",
    "SEQUENCE_NUM = 7\n",
    "\n",
    "ind_start = 0\n",
    "ind_end = end_inds[SEQUENCE_NUM]\n",
    "minor_voxel_size = 0.05\n",
    "major_voxel_size = 0.35\n",
    "chunk_size = np.array([25, 25, 25]) #meters\n",
    "overlap = 3 #meters\n",
    "ground_segmentation_method = 'patchwork' \n",
    "NCUT_ground = False \n",
    "out_folder_ncuts = 'test_data/'\n",
    "if os.path.exists(out_folder_ncuts):\n",
    "        shutil.rmtree(out_folder_ncuts)\n",
    "os.makedirs(out_folder_ncuts)\n",
    "\n",
    "out_chunks = 'pcd_preprocessed/output_chunks/'\n",
    "\n",
    "dataset = create_kitti_odometry_dataset(DATASET_PATH,SEQUENCE_NUM,ncuts_mode=True)\n",
    "\n",
    "out_folder = 'pcd_preprocessed/'\n",
    "if os.path.exists(out_folder) == False : \n",
    "        os.makedirs(out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can be ignored after first run as outputs are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data if already stored \n",
    "\n",
    "if os.path.exists(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd') == False:\n",
    "        pcd_ground_minor, pcd_nonground_minor,\\\n",
    "                all_poses, T_pcd, first_position,kitti_labels = load_and_downsample_point_clouds(out_folder,SEQUENCE_NUM,minor_voxel_size,\\\n",
    "                                                                        ground_mode=ground_segmentation_method)\n",
    "        #o3d.visualization.draw_geometries([color_pcd_by_labels(pcd_nonground_minor,kitti_labels['seg_nonground'])])\n",
    "        o3d.io.write_point_cloud(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd', pcd_ground_minor, write_ascii=False, compressed=False, print_progress=False)\n",
    "        o3d.io.write_point_cloud(f'{out_folder}pcd_nonground_minor{SEQUENCE_NUM}_0.pcd', pcd_nonground_minor, write_ascii=False, compressed=False, print_progress=False)\n",
    "        np.savez(f'{out_folder}kitti_labels_preprocessed{SEQUENCE_NUM}_0.npz',\n",
    "                                                instance_nonground=kitti_labels['instance_nonground'],\n",
    "                                                instance_ground=kitti_labels['instance_ground'],\n",
    "                                                seg_ground = kitti_labels['seg_ground'],\n",
    "                                                seg_nonground=kitti_labels['seg_nonground']\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_ground_minor = o3d.io.read_point_cloud(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd')\n",
    "pcd_nonground_minor = o3d.io.read_point_cloud(f'{out_folder}pcd_nonground_minor{SEQUENCE_NUM}_0.pcd')\n",
    "print(pcd_ground_minor)\n",
    "kitti_labels_orig = {}\n",
    "with np.load(f'{out_folder}kitti_labels_preprocessed{SEQUENCE_NUM}_0.npz') as data :\n",
    "        kitti_labels_orig['instance_ground'] = data['instance_ground']\n",
    "        kitti_labels_orig['instance_nonground'] = data['instance_nonground']\n",
    "        kitti_labels_orig['seg_nonground'] = data['seg_nonground']\n",
    "        kitti_labels_orig['seg_ground'] = data['seg_ground']\n",
    "\n",
    "        \n",
    "\n",
    "with np.load(f'{out_folder}all_poses_{SEQUENCE_NUM}_0.npz') as data:\n",
    "        all_poses = data['all_poses']\n",
    "        T_pcd = data['T_pcd']\n",
    "        first_position = T_pcd[:3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pcd_new = o3d.geometry.PointCloud()\n",
    "pts_num = 1000000\n",
    "pcd_new.points = o3d.utility.Vector3dVector(np.asarray(pcd_nonground_minor.points)[:pts_num])\n",
    "\n",
    "map_labelled = color_pcd_by_labels(pcd_new,\\\n",
    "                kitti_labels['panoptic_nonground'][:pts_num].reshape(-1,1))\n",
    "\n",
    "o3d.visualization.draw_geometries([map_labelled])\n",
    "#o3d.io.write_point_cloud('labelled_map07.pcd',map_labelled)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we subsample the poses based on a voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{out_folder}subsampled_data{str(SEQUENCE_NUM)}_0.npz') == False : \n",
    "\tprint(f'{out_folder}subsampled_data{str(SEQUENCE_NUM)}_0.npz')\n",
    "\tposes, positions, \\\n",
    "\tsampled_indices_local, sampled_indices_global = subsample_and_extract_positions(all_poses,ind_start=ind_start,sequence_num=SEQUENCE_NUM,out_folder=out_folder)\n",
    "\n",
    "\n",
    "with np.load(f'{out_folder}subsampled_data{SEQUENCE_NUM}_0.npz') as data:\n",
    "\tposes=data['poses']\n",
    "\tpositions=data['positions']\n",
    "\tsampled_indices_local = data['sampled_indices_local']\n",
    "\tsampled_indices_global=data['sampled_indices_global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the point cloud into chunks based on a tbd chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_nonground_chunks, pcd_ground_chunks,\\\n",
    "pcd_nonground_chunks_major_downsampling, pcd_ground_chunks_major_downsampling, \\\n",
    "indices,indices_ground, center_positions, \\\n",
    "center_ids, chunk_bounds, kitti_labels = chunk_and_downsample_point_clouds(pcd_nonground_minor, pcd_ground_minor, T_pcd, positions, \n",
    "                                                            first_position, sampled_indices_global, chunk_size=chunk_size, \n",
    "                                                            overlap=overlap, major_voxel_size=major_voxel_size,kitti_labels=kitti_labels_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "import hdbscan\n",
    "\n",
    "#cvc clustering setup \n",
    "#params = [2,0.4,1.5]\n",
    "params = [0.1,0.2,0.5]\n",
    "cvc = pycluster.CVC_cluster(params)\n",
    "\n",
    "def uniform_down_sample_with_indices(points, every_k_points):\n",
    "        # Create a new point cloud for the downsampled output\n",
    "\n",
    "        # List to hold the indices of the points that are kept\n",
    "        indices = []\n",
    "\n",
    "        # Iterate over the points and keep every k-th point\n",
    "        for i in range(0, points.shape[0], every_k_points):\n",
    "            indices.append(i)\n",
    "\n",
    "        return indices\n",
    "\n",
    "def downsample_chunk(points):\n",
    "        num_points_to_sample = 30000\n",
    "        every_k_points = int(\n",
    "            points.shape[0] /\n",
    "            num_points_to_sample)\n",
    "        indeces = uniform_down_sample_with_indices(\n",
    "            points, every_k_points)\n",
    "\n",
    "\n",
    "        return points[indeces]\n",
    "\n",
    "def clustering_logic(pcd_nonground_chunk, pcd_ground_chunk,\n",
    "                        eps=0.3, min_samples=10,method='hdbscan'):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on the point cloud data.\n",
    "\n",
    "    :param cur_pcd: Current point cloud for clustering.\n",
    "    :param pcd_all: All point cloud data.\n",
    "    :param eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    :param min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    :return: Cluster labels for each point in the point cloud.\n",
    "    \"\"\"\n",
    "    \n",
    "    inliers = get_statistical_inlier_indices(pcd_ground_chunk)\n",
    "    ground_inliers = get_subpcd(pcd_ground_chunk, inliers)\n",
    "    mean_hight = np.mean(np.asarray(ground_inliers.points)[:,2])\n",
    "    in_idcs = np.where(np.asarray(ground_inliers.points)[:,2] < (mean_hight + 0.2))[0]\n",
    "    cut_hight = get_subpcd(ground_inliers, in_idcs)\n",
    "    cut_hight.paint_uniform_color([0, 0, 0])\n",
    "    \n",
    "    in_idcs = None\n",
    "    \n",
    "    #in_idcs = np.where(np.asarray(pcd_nonground_chunk.points)[:,2] > (mean_hight + 0.05))[0]\n",
    "    #pcd_nonground_corrected = get_subpcd(pcd_nonground_chunk, in_idcs)\n",
    "    pcd_nonground_corrected = pcd_nonground_chunk\n",
    "    \n",
    "    pcd_nonground_downsampled = o3d.geometry.PointCloud()\n",
    "    pts_downsampled = downsample_chunk(np.asarray(pcd_nonground_corrected.points))\n",
    "    pcd_nonground_downsampled.points = o3d.utility.Vector3dVector(pts_downsampled)\n",
    "    \n",
    "    #clustering = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    #clustering = HDBSCAN(min_cluster_size=10).fit(pts_downsampled)\n",
    "    if method == 'hdbscan': \n",
    "        clustering = hdbscan.HDBSCAN(algorithm='best', alpha=1., approx_min_span_tree=True,\n",
    "                                    gen_min_span_tree=True, leaf_size=100,\n",
    "                                    metric='euclidean', min_cluster_size=10, min_samples=None\n",
    "                                )\n",
    "        clustering.fit(pts_downsampled)\n",
    "        \n",
    "        labels_not_road = clustering.labels_\n",
    "        \n",
    "    elif method == 'felzenswalb':    \n",
    "        pcd_nonground_downsampled.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "        #print(\"estimated normals\")\n",
    "        ## Optionally, you can orient the normals\n",
    "        #o3d.geometry.PointCloud.orient_normals_consistent_tangent_plane(pcd, k=10)\n",
    "        #print(\"create mesh\")\n",
    "        # Apply Poisson reconstruction\n",
    "        mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd_nonground_downsampled, depth=9)\n",
    "        #print(\"converted\")\n",
    "        # Optionally, you can remove low density vertices\n",
    "        #print(\"Mesh remove\")\n",
    "        vertices_to_remove = densities < np.quantile(densities, 0.05)\n",
    "        mesh.remove_vertices_by_mask(vertices_to_remove)\n",
    "\n",
    "        normals = np.asarray(mesh.vertex_normals)\n",
    "        norm_colors = (normals - normals.min(axis=0)) / (normals.max(axis=0) - normals.min(axis=0))\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(norm_colors)\n",
    "\n",
    "        vertices = np.array(mesh.vertices).astype(np.single)\n",
    "        colors = np.array(mesh.vertex_colors).astype(np.single)\n",
    "        faces = np.array(mesh.triangles).astype(np.intc)\n",
    "        \n",
    "        o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "        min_vert_num = 2000\n",
    "\n",
    "        comps, connectivity = felzenszwalb_cpp.segment_mesh(vertices, faces, colors, 0.5, min_vert_num)  # orig was min_vert_num=50\n",
    "        \n",
    "        \n",
    "        # Filter out small segments and floaters\n",
    "        mesh_points = np.array(mesh.vertices)\n",
    "        vertices_tree = KDTree(mesh_points)\n",
    "        segment_ids, segment_counts = np.unique(comps, return_counts=True)\n",
    "        \n",
    "\n",
    "        filtered_comps = comps.copy()\n",
    "        for segment_id, segment_count in zip(segment_ids, segment_counts):\n",
    "                \n",
    "                if (segment_id not in connectivity) or (segment_count < min_vert_num):\n",
    "                        tmp = segment_id.copy()\n",
    "                        _, closest_point_ids = vertices_tree.query(mesh_points[comps == segment_id][0], k=segment_count+1)\n",
    "                        target_segment_id = comps[closest_point_ids][np.nonzero(comps[closest_point_ids] - tmp)[0][0]]\n",
    "\n",
    "                        # update at location \n",
    "                        filtered_comps[comps == segment_id] = target_segment_id\n",
    "\n",
    "        seg_connectivity = connectivity\n",
    "        # Associate each point to a segment\n",
    "        kdtree = KDTree(vertices)\n",
    "        _, idx = kdtree.query(pts_downsampled)\n",
    "        labels_not_road = filtered_comps[idx]\n",
    "        print(\"pts shape\",pts_downsampled.shape)\n",
    "        print('labels shape',labels_not_road.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "    else : \n",
    "        capr = None\n",
    "        hash_table = None\n",
    "        cluster_indices = None\n",
    "        cluster_id = None\n",
    "        capr = cvc.calculateAPR(pts_downsampled)\n",
    "        hash_table = cvc.build_hash_table(capr)\n",
    "        cluster_indices = cvc.cluster(hash_table,capr)\n",
    "        cluster_id = cvc.most_frequent_value(cluster_indices)\n",
    "        labels_not_road = np.ones((pts_downsampled.shape[0], 1)) * -1\n",
    "        \n",
    "        for i in range(len(cluster_id)):\n",
    "                for j in range(len(cluster_indices)):\n",
    "                        if cluster_indices[j] == cluster_id[i]:\n",
    "                                ##append point to cloud with certain colour \n",
    "                                labels_not_road[j] = cluster_id[i]\n",
    "                                #pt_colors[nonground_idcs[j]] = color  \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "        #labels_not_road = np.asarray(cluster_indices) \n",
    "        \n",
    "    colors_gen = generate_random_colors(5000)\n",
    "    \n",
    "    # Reproject cluster labels to the original point cloud size\n",
    "    cluster_labels = np.ones((len(pcd_nonground_corrected.points), 1)) * -1\n",
    "    labels_non_ground = kDTree_1NN_feature_reprojection(cluster_labels, pcd_nonground_corrected, labels_not_road.reshape(-1,1), pcd_nonground_downsampled )\n",
    "    colors = np.zeros((labels_non_ground.shape[0],3))\n",
    "    unique_labels = list(np.unique(labels_non_ground))\n",
    "    \n",
    "    for j in unique_labels:\n",
    "            cur_idcs = np.where(labels_non_ground == j)[0]\n",
    "            \n",
    "            colors[cur_idcs] = np.array(colors_gen[unique_labels.index(j)])\n",
    "        \n",
    "    pcd_nonground_corrected.colors = o3d.utility.Vector3dVector(colors / 255.)\n",
    "    \n",
    "    #o3d.visualization.draw_geometries([pcd_nonground_corrected])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pcd_nonground_corrected, cut_hight, in_idcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_pcd_by_labels(pcd, labels,colors=None,gt_labels=None):\n",
    "    \n",
    "    if colors == None : \n",
    "        colors = generate_random_colors(2000)\n",
    "    pcd_colored = copy.deepcopy(pcd)\n",
    "    pcd_colors = np.zeros(np.asarray(pcd.points).shape)\n",
    "    if gt_labels is None :\n",
    "    \tunique_labels = list(np.unique(labels)) \n",
    "    else: \n",
    "        unique_labels = list(np.unique(gt_labels))\n",
    "    \n",
    "    background_color = np.array([0,0,0])\n",
    "\n",
    "\n",
    "    #for i in range(len(pcd_colored.points)):\n",
    "    for i in unique_labels:\n",
    "        if i == -1 : \n",
    "            continue\n",
    "        idcs = np.where(labels == i)\n",
    "        idcs = idcs[0]\n",
    "        if i == 0 : \n",
    "            pcd_colors[idcs] = background_color\n",
    "        else : \n",
    "            pcd_colors[idcs] = np.array(colors[unique_labels.index(i)])\n",
    "        \n",
    "        #if labels[i] != (-1):\n",
    "        #    pcd_colored.colors[i] = np.array(colors[labels[i]]) / 255\n",
    "    pcd_colored.colors = o3d.utility.Vector3dVector(pcd_colors/ 255)\n",
    "    return pcd_colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = generate_random_colors_map(600)\n",
    "method = 'hdbscan'\n",
    "\n",
    "out_dbscan = out_chunks + 'out_dbscan' + str(SEQUENCE_NUM) + '/'\n",
    "if os.path.exists(out_dbscan) == True : \n",
    "        shutil.rmtree(out_dbscan)\n",
    "        \n",
    "os.makedirs(out_dbscan)\n",
    "\n",
    "\n",
    "limit = -1 ##use this for experiments to run limit chunks numberss\n",
    "\n",
    "\n",
    "patchwise_indices = indices_per_patch(T_pcd, center_positions, positions, first_position, sampled_indices_global, chunk_size)\n",
    "out_data = []\n",
    "for sequence in tqdm(range(len(center_ids))):\n",
    "                obstacle_chunk, ground_chunk, in_idcs = clustering_logic(pcd_nonground_chunks[sequence],pcd_ground_chunks[sequence],\n",
    "                        eps=0.4, min_samples=10,method=method)\n",
    "                \n",
    "                #kitti_chunk_instance = color_pcd_by_labels(obstacle_chunk,kitti_labels['nonground']['instance'][sequence][in_idcs].reshape(-1,),\n",
    "                \n",
    "                #o3d.visualization.draw_geometries([obstacle_chunk + ground_chunk])\n",
    "                #print(kitti_chunk_instance,obstacle_chunk)\n",
    "                \n",
    "                name =  str(center_ids[sequence]).zfill(6) + '.pcd'\n",
    "                \n",
    "                o3d.io.write_point_cloud(out_dbscan + name, obstacle_chunk + ground_chunk, write_ascii=False, compressed=False, print_progress=False)\n",
    "                #o3d.io.write_point_cloud(out_kitti + name, kitti_chunk + pcd_chunk_ground, write_ascii=False, compressed=False, print_progress=False)\n",
    "                #o3d.io.write_point_cloud(out_kitti_instance + name, kitti_chunk_instance + ground_chunk, write_ascii=False, compressed=False, print_progress=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def get_merge_pcds(out_folder_ncuts):\n",
    "        point_clouds = []\n",
    "\n",
    "        # List all files in the folder\n",
    "        files = os.listdir(out_folder_ncuts)\n",
    "        files.sort()\n",
    "\n",
    "        # Filter files with a .pcd extension\n",
    "        pcd_files = [file for file in files if file.endswith(\".pcd\")]\n",
    "        # Load each point cloud and append to the list\n",
    "        for pcd_file in pcd_files:\n",
    "                file_path = os.path.join(out_folder_ncuts, pcd_file)\n",
    "                point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "                point_clouds.append(point_cloud)\n",
    "        return point_clouds\n",
    "\n",
    "def merge_unite_gt(chunks):\n",
    "    last_chunk = chunks[0] \n",
    "    merge = o3d.geometry.PointCloud()\n",
    "    merge += last_chunk\n",
    "\n",
    "    for new_chunk in chunks[1:]:\n",
    "        merge += new_chunk\n",
    "    \n",
    "    merge.remove_duplicated_points()\n",
    "    return merge\n",
    "    \n",
    "def intersect(pred_indices, gt_indices):\n",
    "        intersection = np.intersect1d(pred_indices, gt_indices)\n",
    "        return intersection.size / pred_indices.shape[0]\n",
    "\n",
    "\n",
    "def intersect(pred_indices, gt_indices):\n",
    "        intersection = np.intersect1d(pred_indices, gt_indices)\n",
    "        return intersection.size / pred_indices.shape[0]\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_batch(unique_pred, preds, labels, gt_idcs, threshold, new_ncuts_labels):\n",
    "    pred_idcs = np.where(preds == unique_pred)[0]\n",
    "    cur_intersect = np.sum(np.isin(pred_idcs, gt_idcs))\n",
    "    if cur_intersect > threshold * len(pred_idcs): \n",
    "        new_ncuts_labels[pred_idcs] = 0\n",
    "\n",
    "def remove_semantics(labels, preds, threshold=0.8, num_threads=None):\n",
    "    gt_idcs = np.where(labels == 0)[0]\n",
    "    new_ncuts_labels = preds.copy()\n",
    "    unique_preds = np.unique(preds)\n",
    "    \n",
    "    if num_threads is None:\n",
    "        num_threads = min(len(unique_preds), 8)  # Default to 8 threads if not specified\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for i in tqdm(unique_preds):\n",
    "            futures.append(executor.submit(process_batch, i, preds, labels, gt_idcs, threshold, new_ncuts_labels))\n",
    "        \n",
    "        # Wait for all tasks to complete\n",
    "        for future in tqdm(futures, total=len(futures), desc=\"Processing\"):\n",
    "            future.result()  # Get the result to catch any exceptions\n",
    "        \n",
    "    return new_ncuts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dbscan = out_chunks + 'out_dbscan' + str(SEQUENCE_NUM) + '/' \n",
    "point_clouds_dbscan = get_merge_pcds(out_dbscan)[:-1]\n",
    "#point_clouds_kitti = get_merge_pcds(out_kitti)[:-1]\n",
    "#point_clouds_kitti_instances = get_merge_pcds(out_kitti_instance)[:-1]\n",
    "#merge_kitti = merge_unite_gt(point_clouds_kitti)\n",
    "#merge_kitti_instance = merge_unite_gt(point_clouds_kitti_instances)\n",
    "#o3d.io.write_point_cloud(out_folder + 'kitti_labels.pcd',merge_kitti_instance)\n",
    "\n",
    "merge_kitti_instance = o3d.io.read_point_cloud(out_folder + 'merge_part_kitti_instance' + str(SEQUENCE_NUM) + '.pcd')\n",
    "merge_dbscan = merge_chunks_unite_instances(point_clouds_dbscan)\n",
    "o3d.io.write_point_cloud(out_folder + 'hdbscan_all' + str(SEQUENCE_NUM) + '.pcd',merge_dbscan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "unique_colors, labels_dbscan = np.unique(np.asarray(merge_dbscan.colors), axis=0, return_inverse=True)\n",
    "unique_colors, labels_kitti = np.unique(np.asarray(merge_kitti_instance.colors),axis=0, return_inverse=True)\n",
    "\n",
    "def intersect(pred_indices, gt_indices):\n",
    "        intersection = np.intersect1d(pred_indices, gt_indices)\n",
    "        return intersection.size / pred_indices.shape[0]\n",
    "\n",
    "\n",
    "def remove_semantics(labels,preds):\n",
    "        gt_idcs = np.where(labels == 0)[0]\n",
    "        new_ncuts_labels = preds.copy()\n",
    "        for i in np.unique(preds):\n",
    "                pred_idcs = np.where(preds == i)[0]\n",
    "                cur_intersect = intersect(pred_idcs,gt_idcs)\n",
    "                if cur_intersect > 0.8:\n",
    "                        new_ncuts_labels[pred_idcs] = 0\n",
    "        return new_ncuts_labels\n",
    "\n",
    "new_dbscan_labels = remove_semantics(labels_kitti,labels_dbscan)\n",
    "\n",
    "\n",
    "metrics_dbscan = Metrics(name='dbscan')\n",
    "metrics_test = Metrics(name='test')\n",
    "\n",
    "\n",
    "metrics_dbscan.update_stats(new_dbscan_labels,labels_kitti)\n",
    "#metrics_dbscan.compute_all_aps()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_dbscan_instances = color_pcd_by_labels(merge_dbscan,new_dbscan_labels)\n",
    "merge_kitti_instance = o3d.io.read_point_cloud(out_folder + 'merge_part_kitti_instance' + str(SEQUENCE_NUM) + '.pcd')\n",
    "merge_dbscan = o3d.io.read_point_cloud(out_folder + 'hdbscan_all' + str(SEQUENCE_NUM) + '.pcd')\n",
    "\n",
    "unique_colors, labels_dbscan = np.unique(np.asarray(merge_dbscan.colors), axis=0, return_inverse=True)\n",
    "unique_colors, labels_kitti = np.unique(np.asarray(merge_kitti_instance.colors),axis=0, return_inverse=True)\n",
    "\n",
    "\n",
    "print(labels_kitti,labels_dbscan)\n",
    "new_dbscan_labels = remove_semantics(labels_kitti,labels_dbscan)\n",
    "\n",
    "\n",
    "metrics_dbscan = Metrics(name='dbscan')\n",
    "\n",
    "\n",
    "metrics_dbscan.update_stats(new_dbscan_labels,labels_kitti,calc_all=True)\n",
    "\n",
    "\n",
    "#o3d.visualization.draw_geometries([merge_dbscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = [1.0,1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "recall = [0,0.06666666666666667, 0.13333333333333333, 0.2, 0.26666666666666666, 0.3333333333333333, 0.4, 0.4666666666666667, 0.5333333333333333, 0.6, 0.6666666666666666, 0.7333333333333333, 0.8, 0.8666666666666667, 0.9333333333333333,1.0]\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(recall,prec)\n",
    "plt.show()\n",
    "prec = np.asarray(prec)\n",
    "recall = np.asarray(recall)\n",
    "print(np.trapz(prec,recall)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
