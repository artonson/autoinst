{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "%matplotlib inline \n",
    "\n",
    "src_path = os.path.abspath(\"../..\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "%load_ext autoreload\n",
    "from dataset.kitti_odometry_dataset import KittiOdometryDataset, KittiOdometryDatasetConfig\n",
    "from dataset.filters.filter_list import FilterList\n",
    "from dataset.filters.kitti_gt_mo_filter import KittiGTMovingObjectFilter\n",
    "from dataset.filters.range_filter import RangeFilter\n",
    "from dataset.filters.apply_pose import ApplyPose\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from normalized_cut import normalized_cut\n",
    "from ncuts_utils import ncuts_chunk,kDTree_1NN_feature_reprojection_colors, get_merge_pcds\n",
    "from dataset_utils import * \n",
    "from point_cloud_utils import get_pcd, transform_pcd, kDTree_1NN_feature_reprojection, remove_isolated_points, get_subpcd, get_statistical_inlier_indices, merge_chunks_unite_instances\n",
    "from aggregate_pointcloud import aggregate_pointcloud\n",
    "from visualization_utils import generate_random_colors, color_pcd_by_labels\n",
    "from sam_label_distace import sam_label_distance\n",
    "from chunk_generation import subsample_positions, chunks_from_pointcloud, indices_per_patch, tarl_features_per_patch, image_based_features_per_patch, dinov2_mean, get_indices_feature_reprojection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset depending on kitti sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/Users/cedric/Datasets/semantic_kitti/')\n",
    "SEQUENCE_NUM = 7\n",
    "\n",
    "ind_start = 0\n",
    "ind_end = 1100\n",
    "minor_voxel_size = 0.05\n",
    "major_voxel_size = 0.35\n",
    "chunk_size = np.array([25, 25, 25]) #meters\n",
    "overlap = 3 #meters\n",
    "ground_segmentation_method = 'patchwork' \n",
    "NCUT_ground = False \n",
    "out_folder_ncuts = 'test_data/'\n",
    "\n",
    "dataset = create_kitti_odometry_dataset(DATASET_PATH,SEQUENCE_NUM,ncuts_mode=True)\n",
    "\n",
    "out_folder = 'pcd_preprocessed/'\n",
    "if os.path.exists(out_folder) == False : \n",
    "        os.makedirs(out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we aggregate a large point cloud based on (ind_start, ind_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchWorkpp::PatchWorkpp() - INITIALIZATION COMPLETE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_and_save_point_clouds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mind_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43mind_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43mminor_voxel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminor_voxel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmajor_voxel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmajor_voxel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43micp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mout_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43msequence_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEQUENCE_NUM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mground_segmentation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_segmentation_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/unsup_3d_instances/point-to-pixel-mapping/dataset_utils.py:44\u001b[0m, in \u001b[0;36mprocess_and_save_point_clouds\u001b[0;34m(dataset, ind_start, ind_end, ground_segmentation_method, icp, minor_voxel_size, major_voxel_size, out_folder, sequence_num)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(out_folder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m : \n\u001b[1;32m     42\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(out_folder)\n\u001b[0;32m---> 44\u001b[0m pcd_ground, pcd_nonground, all_poses, T_pcd,kitti_labels \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_pointcloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mind_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_segmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_segmentation_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43micp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43micp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Saving point clouds and poses\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pcd_ground \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m : \n",
      "File \u001b[0;32m~/unsup_3d_instances/point-to-pixel-mapping/aggregate_pointcloud.py:81\u001b[0m, in \u001b[0;36maggregate_pointcloud\u001b[0;34m(dataset, ind_start, ind_end, icp, icp_threshold, ground_segmentation)\u001b[0m\n\u001b[1;32m     79\u001b[0m panoptic_labels \u001b[38;5;241m=\u001b[39m  dataset[i]\u001b[38;5;241m.\u001b[39mpanoptic_labels\n\u001b[1;32m     80\u001b[0m semantic_labels \u001b[38;5;241m=\u001b[39m  dataset[i]\u001b[38;5;241m.\u001b[39msemantic_labels\n\u001b[0;32m---> 81\u001b[0m instance_labels \u001b[38;5;241m=\u001b[39m  \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39minstance_labels\n\u001b[1;32m     85\u001b[0m pcd \u001b[38;5;241m=\u001b[39m get_pcd(dataset[i]\u001b[38;5;241m.\u001b[39mpoint_cloud)\n\u001b[1;32m     86\u001b[0m pose \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_pose(i)\n",
      "File \u001b[0;32m~/unsup_3d_instances/point-to-pixel-mapping/dataset/kitti_odometry_dataset.py:333\u001b[0m, in \u001b[0;36mKittiOdometryDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetEntry:\n\u001b[1;32m    328\u001b[0m     entry \u001b[38;5;241m=\u001b[39m DatasetEntry(\n\u001b[1;32m    329\u001b[0m         index,\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pose(index),\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_point_cloud(index),\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_intensity(index),\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_panoptic_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_semantic_labels(index),\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_instance_labels(index),\n\u001b[1;32m    336\u001b[0m         {\n\u001b[1;32m    337\u001b[0m             cam_name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_image(cam_name, index)\n\u001b[1;32m    338\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m cam_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera_names\n\u001b[1;32m    339\u001b[0m         },\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfilters:\n\u001b[1;32m    343\u001b[0m         entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfilters(entry, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_and_save_point_clouds(dataset,ind_start,ind_end,minor_voxel_size=minor_voxel_size,\n",
    "                              major_voxel_size=major_voxel_size,icp=False,\n",
    "                              out_folder=out_folder,sequence_num=SEQUENCE_NUM,\n",
    "                              ground_segmentation_method=ground_segmentation_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data if already stored \n",
    "pcd_ground_minor, pcd_nonground_minor,pcd_ground, pcd_nonground,\\\n",
    "\tall_poses, T_pcd, first_position,kitti_labels = load_and_downsample_point_clouds(out_folder,SEQUENCE_NUM,minor_voxel_size,\\\n",
    "                                                                     ground_mode=ground_segmentation_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlabel_cat = np.vstack((kitti_labels['panoptic_nonground'].reshape(-1,1),kitti_labels['panoptic_ground'].reshape(-1,1)))\\n\\npcd_new = o3d.geometry.PointCloud()\\npts_num = 10000000\\npcd_new.points = o3d.utility.Vector3dVector(np.asarray(pcd_nonground.points)[:pts_num])\\n\\n\\nmap_labelled = color_pcd_by_labels(pcd_new,                kitti_labels['panoptic_nonground'][:pts_num].reshape(-1,1))\\n\\no3d.visualization.draw_geometries([map_labelled])\\n#o3d.io.write_point_cloud('labelled_map07.pcd',map_labelled)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "label_cat = np.vstack((kitti_labels['panoptic_nonground'].reshape(-1,1),kitti_labels['panoptic_ground'].reshape(-1,1)))\n",
    "\n",
    "pcd_new = o3d.geometry.PointCloud()\n",
    "pts_num = 10000000\n",
    "pcd_new.points = o3d.utility.Vector3dVector(np.asarray(pcd_nonground.points)[:pts_num])\n",
    "\n",
    "\n",
    "map_labelled = color_pcd_by_labels(pcd_new,\\\n",
    "                kitti_labels['panoptic_nonground'][:pts_num].reshape(-1,1))\n",
    "\n",
    "o3d.visualization.draw_geometries([map_labelled])\n",
    "#o3d.io.write_point_cloud('labelled_map07.pcd',map_labelled)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we subsample the poses based on a voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses, positions, \\\n",
    "sampled_indices_local, sampled_indices_global = subsample_and_extract_positions(all_poses,ind_start=ind_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the point cloud into chunks based on a tbd chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled from (523016, 3) to (7643, 3) points (non-ground)\n",
      "Downsampled from (257407, 3) to (5501, 3) points (ground)\n",
      "Downsampled from (392323, 3) to (6659, 3) points (non-ground)\n",
      "Downsampled from (273555, 3) to (4539, 3) points (ground)\n",
      "Downsampled from (429859, 3) to (7470, 3) points (non-ground)\n",
      "Downsampled from (291612, 3) to (6157, 3) points (ground)\n",
      "Downsampled from (381642, 3) to (5813, 3) points (non-ground)\n",
      "Downsampled from (255212, 3) to (4655, 3) points (ground)\n",
      "Downsampled from (414475, 3) to (7677, 3) points (non-ground)\n",
      "Downsampled from (237620, 3) to (4305, 3) points (ground)\n",
      "Downsampled from (463157, 3) to (8880, 3) points (non-ground)\n",
      "Downsampled from (226253, 3) to (5086, 3) points (ground)\n",
      "Downsampled from (506123, 3) to (9688, 3) points (non-ground)\n",
      "Downsampled from (228215, 3) to (4918, 3) points (ground)\n",
      "Downsampled from (389712, 3) to (6633, 3) points (non-ground)\n",
      "Downsampled from (308744, 3) to (4290, 3) points (ground)\n",
      "Downsampled from (280190, 3) to (4670, 3) points (non-ground)\n",
      "Downsampled from (445550, 3) to (6148, 3) points (ground)\n",
      "Downsampled from (317852, 3) to (6098, 3) points (non-ground)\n",
      "Downsampled from (379220, 3) to (5586, 3) points (ground)\n",
      "Downsampled from (387816, 3) to (7721, 3) points (non-ground)\n",
      "Downsampled from (359782, 3) to (5497, 3) points (ground)\n",
      "Downsampled from (184448, 3) to (3774, 3) points (non-ground)\n",
      "Downsampled from (389151, 3) to (5520, 3) points (ground)\n",
      "Downsampled from (199395, 3) to (4127, 3) points (non-ground)\n",
      "Downsampled from (416036, 3) to (5994, 3) points (ground)\n",
      "Downsampled from (408664, 3) to (6446, 3) points (non-ground)\n",
      "Downsampled from (375810, 3) to (6250, 3) points (ground)\n",
      "Downsampled from (476070, 3) to (8020, 3) points (non-ground)\n",
      "Downsampled from (303409, 3) to (6179, 3) points (ground)\n",
      "Downsampled from (382381, 3) to (6338, 3) points (non-ground)\n",
      "Downsampled from (335270, 3) to (5700, 3) points (ground)\n",
      "Downsampled from (330746, 3) to (6146, 3) points (non-ground)\n",
      "Downsampled from (327581, 3) to (5693, 3) points (ground)\n",
      "Downsampled from (406052, 3) to (7046, 3) points (non-ground)\n",
      "Downsampled from (206707, 3) to (4643, 3) points (ground)\n",
      "Downsampled from (422976, 3) to (7520, 3) points (non-ground)\n",
      "Downsampled from (209102, 3) to (4738, 3) points (ground)\n",
      "Downsampled from (401315, 3) to (6104, 3) points (non-ground)\n",
      "Downsampled from (363511, 3) to (5407, 3) points (ground)\n",
      "Downsampled from (300792, 3) to (4939, 3) points (non-ground)\n",
      "Downsampled from (396420, 3) to (6060, 3) points (ground)\n",
      "Downsampled from (313196, 3) to (5781, 3) points (non-ground)\n",
      "Downsampled from (251931, 3) to (5086, 3) points (ground)\n",
      "Downsampled from (309416, 3) to (5698, 3) points (non-ground)\n",
      "Downsampled from (274970, 3) to (5225, 3) points (ground)\n",
      "Downsampled from (274156, 3) to (4913, 3) points (non-ground)\n",
      "Downsampled from (284435, 3) to (4914, 3) points (ground)\n",
      "Downsampled from (295151, 3) to (5374, 3) points (non-ground)\n",
      "Downsampled from (254408, 3) to (4843, 3) points (ground)\n",
      "Downsampled from (298362, 3) to (5378, 3) points (non-ground)\n",
      "Downsampled from (349049, 3) to (5351, 3) points (ground)\n",
      "Downsampled from (212601, 3) to (3902, 3) points (non-ground)\n",
      "Downsampled from (434219, 3) to (6130, 3) points (ground)\n",
      "Downsampled from (478729, 3) to (7473, 3) points (non-ground)\n",
      "Downsampled from (299350, 3) to (5988, 3) points (ground)\n",
      "Downsampled from (552680, 3) to (7790, 3) points (non-ground)\n",
      "Downsampled from (327195, 3) to (5910, 3) points (ground)\n",
      "Downsampled from (603452, 3) to (8669, 3) points (non-ground)\n",
      "Downsampled from (428943, 3) to (6376, 3) points (ground)\n"
     ]
    }
   ],
   "source": [
    "pcd_nonground_chunks, pcd_ground_chunks,\\\n",
    "pcd_nonground_chunks_major_downsampling, pcd_ground_chunks_major_downsampling, \\\n",
    "indices,indices_ground, center_positions, \\\n",
    "center_ids, chunk_bounds, kitti_labels = chunk_and_downsample_point_clouds(pcd_nonground_minor, pcd_ground_minor, T_pcd, positions, \n",
    "                                                            first_position, sampled_indices_global, chunk_size=chunk_size, \n",
    "                                                            overlap=overlap, major_voxel_size=major_voxel_size,kitti_labels=kitti_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def DBSCAN_clustering_logic(cur_pcd, pcd_all, eps=1.0, min_samples=100):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on the point cloud data.\n",
    "\n",
    "    :param cur_pcd: Current point cloud for clustering.\n",
    "    :param pcd_all: All point cloud data.\n",
    "    :param eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    :param min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    :return: Cluster labels for each point in the point cloud.\n",
    "    \"\"\"\n",
    "    not_road_points = np.asarray(cur_pcd.points)\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(not_road_points)\n",
    "    labels_not_road = clustering.labels_\n",
    "    colors_gen = generate_random_colors(500)\n",
    "    \n",
    "    # Reproject cluster labels to the original point cloud size\n",
    "    cluster_labels = np.ones((len(pcd_all.points), 1)) * -1\n",
    "    labels_non_ground = kDTree_1NN_feature_reprojection(cluster_labels, pcd_all, labels_not_road.reshape(-1,1), cur_pcd)\n",
    "    colors = np.zeros((labels_non_ground.shape[0],3))\n",
    "    unique_labels = list(np.unique(labels_non_ground))\n",
    "    for j in unique_labels:\n",
    "            cur_idcs = np.where(labels_non_ground == j)[0]\n",
    "            \n",
    "            colors[cur_idcs] = np.array(colors_gen[unique_labels.index(j)])\n",
    "    pcd_all.colors = o3d.utility.Vector3dVector(colors / 255.)\n",
    "    return pcd_all\n",
    "\n",
    "def dbscan_clustering(pcd_chunks_major_downsampled, pcds, center_ids,ground_clouds):\n",
    "    labels_clustering = []\n",
    "    for i in range(len(pcd_chunks_major_downsampled)):\n",
    "        cur_pcd = pcd_chunks_major_downsampled[i]\n",
    "        pcd_all = pcds[i]\n",
    "        ground_cloud = ground_clouds[i]\n",
    "        cluster_labels = DBSCAN_clustering_logic(cur_pcd, pcd_all)  # Implement your DBSCAN logic here\n",
    "        ground_labels = np.ones((np.asarray(ground_cloud.points).shape[0],1)) * -1\n",
    "        cluster_labels = np.concatenate((cluster_labels,ground_labels),0)\n",
    "        labels_clustering.append(cluster_labels)\n",
    "        \n",
    "    return labels_clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of sequence 0\n",
      "7643 points in downsampled chunk (major)\n",
      "There are 1616 points without TARL features\n",
      "Adjacency Matrix built\n",
      "0 isolated points removed\n",
      "Start of normalized Cuts\n",
      "There are 39 cut regions\n",
      "Ratio of points in top 3 groups: 0.3791704827947141\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.0\n",
    "theta = 0.0\n",
    "beta = 1.0\n",
    "gamma = 0.0\n",
    "proximity_threshold = 1.0\n",
    "out_dbscan = 'out_dbscan/'\n",
    "if os.path.exists(out_dbscan) == False : \n",
    "        os.makedirs(out_dbscan)\n",
    "\n",
    "\n",
    "patchwise_indices = indices_per_patch(T_pcd, center_positions, positions, first_position, sampled_indices_global, chunk_size)\n",
    "out_data = []\n",
    "for sequence in range(len(center_ids))[:1]:\n",
    "        if NCUT_ground == False : \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #o3d.visualization.draw_geometries([pcd_dbscan])\n",
    "                merged_chunk,file_name, pcd_chunk = ncuts_chunk(dataset,indices,pcd_nonground_chunks,pcd_ground_chunks,\n",
    "                        pcd_nonground_chunks_major_downsampling,\n",
    "                        pcd_nonground_minor,T_pcd,center_positions,center_ids,\n",
    "                        positions,first_position,sampled_indices_global,\n",
    "                        chunk_size=chunk_size,major_voxel_size=major_voxel_size,\n",
    "                        alpha=alpha,beta=beta,gamma=gamma,theta=theta,\n",
    "                        proximity_threshold=proximity_threshold,\n",
    "                        out_folder=out_folder_ncuts,ground_mode=False,sequence=sequence,\n",
    "                        patchwise_indices=patchwise_indices)\n",
    "                \n",
    "                name = file_name.split('/')[-1]\n",
    "                o3d.io.write_point_cloud(file_name, pcd_chunk, write_ascii=False, compressed=False, print_progress=False)\n",
    "                \n",
    "                pcd_dbscan = DBSCAN_clustering_logic(pcd_nonground_chunks_major_downsampling[sequence],\n",
    "                                                pcd_nonground_chunks[sequence],\n",
    "                                                eps=0.6, min_samples=10)\n",
    "                o3d.io.write_point_cloud(out_dbscan + name, pcd_dbscan, write_ascii=False, compressed=False, print_progress=False)\n",
    "                \n",
    "\n",
    "                \n",
    "        else : \n",
    "                obstacle_out,file_name = ncuts_chunk(dataset,indices,pcd_nonground_chunks,pcd_ground_chunks,\n",
    "                        pcd_nonground_chunks_major_downsampling,\n",
    "                        pcd_nonground_minor,T_pcd,center_positions,center_ids,\n",
    "                        positions,first_position,sampled_indices_global,\n",
    "                        chunk_size=chunk_size,major_voxel_size=major_voxel_size,\n",
    "                        alpha=alpha,beta=beta,gamma=gamma,theta=theta,\n",
    "                        proximity_threshold=proximity_threshold,\n",
    "                        out_folder=out_folder_ncuts,ground_mode=True,sequence=sequence,\n",
    "                        patchwise_indices=patchwise_indices)\n",
    "                \n",
    "                \n",
    "                ground_out,file_name = ncuts_chunk(dataset,indices_ground,pcd_ground_chunks,None,\n",
    "                        pcd_ground_chunks_major_downsampling,\n",
    "                        pcd_ground_minor,T_pcd,center_positions,center_ids,\n",
    "                        positions,first_position,sampled_indices_global,\n",
    "                        chunk_size=chunk_size,major_voxel_size=major_voxel_size,\n",
    "                        alpha=alpha,beta=beta,gamma=gamma,theta=theta,\n",
    "                        proximity_threshold=proximity_threshold,\n",
    "                        out_folder=out_folder_ncuts,ground_mode=True,sequence=sequence,\n",
    "                        patchwise_indices=patchwise_indices)\n",
    "\n",
    "                o3d.io.write_point_cloud(file_name, obstacle_out + ground_out, write_ascii=False, compressed=False, print_progress=False)\n",
    "\n",
    "                print(\"Pointcloud written to file\")\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000062.pcd', '000090.pcd']\n",
      "['000062.pcd', '000090.pcd']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_clouds = get_merge_pcds(out_folder_ncuts)[:1]\n",
    "point_clouds_dbscan = get_merge_pcds(out_dbscan)[:1]\n",
    "merge = merge_chunks_unite_instances(point_clouds)\n",
    "merge_dbscan = merge_chunks_unite_instances(point_clouds_dbscan)\n",
    "\n",
    "o3d.io.write_point_cloud(out_folder + \"merge_part.pcd\", merge, write_ascii=False, compressed=False, print_progress=False)\n",
    "o3d.io.write_point_cloud(out_folder + \"merge_part_dbscan.pcd\", merge_dbscan, write_ascii=False, compressed=False, print_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 523016 points.\n",
      "(523016, 8, 1)\n",
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "##load merge pcd \n",
    "merge = o3d.io.read_point_cloud(out_folder + 'merge_part.pcd')\n",
    "\n",
    "print(merge)\n",
    "print(np.vstack(kitti_labels['nonground']['panoptic'][:len(point_clouds)]).shape)\n",
    "\n",
    "kitti_gt = color_pcd_by_labels(merge,np.vstack(kitti_labels['nonground']['panoptic'][:len(point_clouds)]))\n",
    "\n",
    "\n",
    "o3d.visualization.draw_geometries([merge.translate([30,0,0]),merge_dbscan.translate([-30,0,0]),kitti_gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/Users/cedric/Datasets/semantic_kitti/')\n",
    "SEQUENCE_NUM = 7\n",
    "\n",
    "dataset = create_kitti_odometry_dataset(DATASET_PATH,SEQUENCE_NUM,ncuts_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open3d.pipelines import registration\n",
    "import numpy as np \n",
    "merge.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.5,max_nn=200))\n",
    "\n",
    "for i in range(70,72):\n",
    "\tlocal_pcd = o3d.geometry.PointCloud()\n",
    "\tlocal_pcd.points = o3d.utility.Vector3dVector(dataset[i].point_cloud[:, :3])\n",
    "\t\n",
    "\t\n",
    "\t## label visualization \n",
    "\tlabeled_pcd = o3d.geometry.PointCloud()\n",
    "\tlabeled_pcd.points = o3d.utility.Vector3dVector(dataset[i].point_cloud[:, :3])\n",
    "\tlabeled_pcd.colors = o3d.utility.Vector3dVector(np.vstack([0,0,0] for i in range(np.asarray(labeled_pcd.points).shape[0])))\n",
    "\tpanoptic_labels = dataset[i].panoptic_labels # semantics + panoptics combined \n",
    "\tsemantic_labels = dataset[i].semantic_labels\n",
    "\tinstance_labels = dataset[i].instance_labels\n",
    "\tintensity = dataset[i].intensity\n",
    "\t\n",
    "\ttransform = dataset.get_pose(i)\n",
    "\tlocal_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.5,max_nn=200))\n",
    "\treg_p2l = registration.registration_icp(local_pcd, merge, 0.9, transform, registration.TransformationEstimationPointToPlane(), registration.ICPConvergenceCriteria(max_iteration=1000))\n",
    "\ttransform = reg_p2l.transformation\n",
    "\tlocal_pcd.transform(transform)\n",
    "\t\n",
    "\tlocal_pcd.normals = o3d.utility.Vector3dVector([])\n",
    "\t\n",
    "\tlocal_pcd.paint_uniform_color([0, 0, 0])\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\tcolors, labels,local_labels = kDTree_1NN_feature_reprojection_colors(np.asarray(local_pcd.colors), local_pcd, np.asarray(merge.colors), merge,panoptic_labels, max_radius=0.2)\n",
    "\tlabeled_pcd = color_pcd_by_labels(labeled_pcd,labels.reshape(-1,))\n",
    "\tlocal_pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\t#o3d.io.write_point_cloud(\"test.pcd\", local_pcd, write_ascii=False, compressed=False, print_progress=False)\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\tlabeled_pcd.translate([0,120,0])\n",
    "\to3d.visualization.draw_geometries([local_pcd,labeled_pcd])\n",
    "\tunique_colors, labels = np.unique(colors, axis=0, return_inverse=True)\n",
    "\tunseen_mask = np.all(colors == [1,0,0], axis=1)\n",
    "\tlabels[unseen_mask] = -1\n",
    "\tstreet_mask = np.all(colors == [0,0,0], axis=1)\n",
    "\tlabels[street_mask] = 1\n",
    "\t\n",
    "\t#np.savez('output_files/7_tarl/ ' + str(i) + '.npz',labels=labels)\n",
    "\n",
    "\t\n",
    "\n",
    "\t#np.savez('output_files/7_original/' + str(i) + '.npz',ncut_labels=local_labels,kitti_labels=labels,pts=np.asarray(local_pcd.points),           \n",
    "\t#intensities=intensity,kitti_labels_semantic=semantic_labels,kitti_labels_instance=instance_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "with np.load('output_files/7_original/0.npz') as data:\n",
    "            xyz = data['pts'].astype(np.float)\n",
    "            labels = data['ncut_labels'].astype(np.int32)  \n",
    "            kitti_labels = data['kitti_labels']\n",
    "            intensity = data['intensities']\n",
    "            \n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "pcd = color_pcd_by_labels(pcd,labels)\n",
    "o3d.visualization.draw_geometries([pcd])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
