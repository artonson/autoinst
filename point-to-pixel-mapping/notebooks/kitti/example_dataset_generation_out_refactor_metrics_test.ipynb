{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "%matplotlib inline \n",
    "\n",
    "src_path = os.path.abspath(\"../..\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "%load_ext autoreload\n",
    "from dataset.kitti_odometry_dataset import KittiOdometryDataset, KittiOdometryDatasetConfig\n",
    "from dataset.filters.filter_list import FilterList\n",
    "from dataset.filters.kitti_gt_mo_filter import KittiGTMovingObjectFilter\n",
    "from dataset.filters.range_filter import RangeFilter\n",
    "from dataset.filters.apply_pose import ApplyPose\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from ncuts_utils import ncuts_chunk,kDTree_1NN_feature_reprojection_colors, get_merge_pcds\n",
    "from dataset_utils import * \n",
    "from point_cloud_utils import get_pcd, transform_pcd, kDTree_1NN_feature_reprojection, remove_isolated_points, get_subpcd, get_statistical_inlier_indices, merge_chunks_unite_instances, merge_chunks_unite_instances2, remove_semantics, get_merge_pcds, merge_unite_gt\n",
    "from aggregate_pointcloud import aggregate_pointcloud\n",
    "from visualization_utils import generate_random_colors, color_pcd_by_labels,generate_random_colors_map\n",
    "from sam_label_distace import sam_label_distance\n",
    "from chunk_generation import subsample_positions, chunks_from_pointcloud, indices_per_patch, tarl_features_per_patch, image_based_features_per_patch, dinov2_mean, get_indices_feature_reprojection\n",
    "from metrics.metrics_class import Metrics\n",
    "import matplotlib.pyplot as plt \n",
    "import shutil\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset depending on kitti sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/media/cedric/Datasets2/semantic_kitti/')\n",
    "end_inds = {0:4541,1:1100,2:4661,3:800,4:271,5:2761,6:1101,7:1100,8:4071,9:1591,10:1201}\n",
    "SEQUENCE_NUM = 7\n",
    "old = False \n",
    "\n",
    "ind_start = 0\n",
    "ind_end = end_inds[SEQUENCE_NUM]\n",
    "minor_voxel_size = 0.05\n",
    "major_voxel_size = 0.35\n",
    "chunk_size = np.array([25, 25, 25]) #meters\n",
    "overlap = 3 #meters\n",
    "ground_segmentation_method = 'patchwork' \n",
    "NCUT_ground = False\n",
    "\n",
    "out_chunks = '../../pcd_preprocessed/output_chunks/'\n",
    "\n",
    "out_folder_ncuts = out_chunks + 'test_data' + str(SEQUENCE_NUM) + '/'\n",
    "if os.path.exists(out_folder_ncuts) == False:\n",
    "        os.makedirs(out_folder_ncuts)\n",
    "\n",
    "dataset = create_kitti_odometry_dataset(DATASET_PATH,SEQUENCE_NUM,ncuts_mode=True)\n",
    "\n",
    "out_folder = '../../pcd_preprocessed/'\n",
    "if os.path.exists(out_folder) == False : \n",
    "        os.makedirs(out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we aggregate a large point cloud based on (ind_start, ind_end)\n",
    "## This cell can be ignored after first run as outputs are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(out_folder + 'all_poses_' + str(SEQUENCE_NUM) + '_' + str(0) + '.npz') == False:\n",
    "        process_and_save_point_clouds(dataset,ind_start,ind_end,minor_voxel_size=minor_voxel_size,\n",
    "                                major_voxel_size=major_voxel_size,icp=False,\n",
    "                                out_folder=out_folder,sequence_num=SEQUENCE_NUM,\n",
    "                                ground_segmentation_method=ground_segmentation_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can be ignored after first run as outputs are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data if already stored \n",
    "\n",
    "if os.path.exists(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd') == False:\n",
    "        pcd_ground_minor, pcd_nonground_minor,\\\n",
    "                all_poses, T_pcd, first_position,kitti_labels = load_and_downsample_point_clouds(out_folder,SEQUENCE_NUM,minor_voxel_size,\\\n",
    "                                                                        ground_mode=ground_segmentation_method)\n",
    "        #o3d.visualization.draw_geometries([color_pcd_by_labels(pcd_nonground_minor,kitti_labels['seg_nonground'])])\n",
    "        o3d.io.write_point_cloud(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd', pcd_ground_minor, write_ascii=False, compressed=False, print_progress=False)\n",
    "        o3d.io.write_point_cloud(f'{out_folder}pcd_nonground_minor{SEQUENCE_NUM}_0.pcd', pcd_nonground_minor, write_ascii=False, compressed=False, print_progress=False)\n",
    "        np.savez(f'{out_folder}kitti_labels_preprocessed{SEQUENCE_NUM}_0.npz',\n",
    "                                                instance_nonground=kitti_labels['instance_nonground'],\n",
    "                                                instance_ground=kitti_labels['instance_ground'],\n",
    "                                                seg_ground = kitti_labels['seg_ground'],\n",
    "                                                seg_nonground=kitti_labels['seg_nonground']\n",
    "                                                )\n",
    "        #o3d.visualization.draw_geometries([pcd_nonground_minor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_ground_minor = o3d.io.read_point_cloud(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd')\n",
    "pcd_nonground_minor = o3d.io.read_point_cloud(f'{out_folder}pcd_nonground_minor{SEQUENCE_NUM}_0.pcd')\n",
    "print(pcd_ground_minor)\n",
    "kitti_labels_orig = {}\n",
    "with np.load(f'{out_folder}kitti_labels_preprocessed{SEQUENCE_NUM}_0.npz') as data :\n",
    "        kitti_labels_orig['instance_ground'] = data['instance_ground']\n",
    "        kitti_labels_orig['instance_nonground'] = data['instance_nonground']\n",
    "        kitti_labels_orig['seg_nonground'] = data['seg_nonground']\n",
    "        kitti_labels_orig['seg_ground'] = data['seg_ground']\n",
    "\n",
    "        \n",
    "\n",
    "with np.load(f'{out_folder}all_poses_{SEQUENCE_NUM}_0.npz') as data:\n",
    "        all_poses = data['all_poses']\n",
    "        T_pcd = data['T_pcd']\n",
    "        first_position = T_pcd[:3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pcd_new = o3d.geometry.PointCloud()\n",
    "pts_num = 1000000\n",
    "pcd_new.points = o3d.utility.Vector3dVector(np.asarray(pcd_nonground_minor.points)[:pts_num])\n",
    "\n",
    "map_labelled = color_pcd_by_labels(pcd_new,\\\n",
    "                kitti_labels['panoptic_nonground'][:pts_num].reshape(-1,1))\n",
    "\n",
    "o3d.visualization.draw_geometries([map_labelled])\n",
    "#o3d.io.write_point_cloud('labelled_map07.pcd',map_labelled)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we subsample the poses based on a voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{out_folder}subsampled_data{str(SEQUENCE_NUM)}_0.npz') == False : \n",
    "\tprint(f'{out_folder}subsampled_data{str(SEQUENCE_NUM)}_0.npz')\n",
    "\tposes, positions, \\\n",
    "\tsampled_indices_local, sampled_indices_global = subsample_and_extract_positions(all_poses,ind_start=ind_start,sequence_num=SEQUENCE_NUM,out_folder=out_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(f'{out_folder}subsampled_data{SEQUENCE_NUM}_0.npz') as data:\n",
    "\tposes=data['poses']\n",
    "\tpositions=data['positions']\n",
    "\tsampled_indices_local = data['sampled_indices_local']\n",
    "\tsampled_indices_global=data['sampled_indices_global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the point cloud into chunks based on a tbd chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_nonground_chunks, pcd_ground_chunks,\\\n",
    "pcd_nonground_chunks_major_downsampling, pcd_ground_chunks_major_downsampling, \\\n",
    "indices,indices_ground, center_positions, \\\n",
    "center_ids, chunk_bounds, kitti_labels, obbs = chunk_and_downsample_point_clouds(dataset,pcd_nonground_minor, pcd_ground_minor, T_pcd, positions, \n",
    "                                                            first_position, sampled_indices_global, chunk_size=chunk_size, \n",
    "                                                            overlap=overlap, major_voxel_size=major_voxel_size,kitti_labels=kitti_labels_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_pcd_by_labels(pcd, labels,colors=None,gt_labels=None,semantics=False):\n",
    "    \n",
    "    if colors == None : \n",
    "        colors = generate_random_colors(2000)\n",
    "    pcd_colored = copy.deepcopy(pcd)\n",
    "    pcd_colors = np.zeros(np.asarray(pcd.points).shape)\n",
    "    if gt_labels is None :\n",
    "    \tunique_labels = list(np.unique(labels)) \n",
    "    else: \n",
    "        unique_labels = list(np.unique(gt_labels))\n",
    "    \n",
    "    background_color = np.array([0,0,0])\n",
    "    #for i in range(len(pcd_colored.points)):\n",
    "    for i in unique_labels:\n",
    "        if i == -1 : \n",
    "            continue\n",
    "        idcs = np.where(labels == i)\n",
    "        idcs = idcs[0]\n",
    "        if i == 0 and semantics == False : \n",
    "            pcd_colors[idcs] = background_color\n",
    "        else : \n",
    "            pcd_colors[idcs] = np.array(colors[unique_labels.index(i)])\n",
    "        \n",
    "    if semantics : \n",
    "        pcd_colored.colors = o3d.utility.Vector3dVector(pcd_colors)\n",
    "    else : \n",
    "        pcd_colored.colors = o3d.utility.Vector3dVector(pcd_colors/255)\n",
    "    return pcd_colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict_normalized = {\n",
    "    0: [0.0, 0.0, 0.0],\n",
    "    1: [0.0, 0.0, 1.0],\n",
    "    10: [0.9607843137254902, 0.5882352941176471, 0.39215686274509803],\n",
    "    11: [0.9607843137254902, 0.9019607843137255, 0.39215686274509803],\n",
    "    13: [0.9803921568627451, 0.3137254901960784, 0.39215686274509803],\n",
    "    15: [0.5882352941176471, 0.23529411764705882, 0.11764705882352941],\n",
    "    16: [1.0, 0.0, 0.0],\n",
    "    18: [0.7058823529411765, 0.11764705882352941, 0.3137254901960784],\n",
    "    20: [1.0, 0.0, 0.0],\n",
    "    30: [0.11764705882352941, 0.11764705882352941, 1.0],\n",
    "    31: [0.7843137254901961, 0.1568627450980392, 1.0],\n",
    "    32: [0.35294117647058826, 0.11764705882352941, 0.5882352941176471],\n",
    "    40: [1.0, 0.0, 1.0],\n",
    "    44: [1.0, 0.5882352941176471, 1.0],\n",
    "    48: [0.29411764705882354, 0.0, 0.29411764705882354],\n",
    "    49: [0.29411764705882354, 0.0, 0.6862745098039216],\n",
    "    50: [0.0, 0.7843137254901961, 1.0],\n",
    "    51: [0.19607843137254902, 0.47058823529411764, 1.0],\n",
    "    52: [0.0, 0.5882352941176471, 1.0],\n",
    "    60: [0.6666666666666666, 1.0, 0.5882352941176471],\n",
    "    70: [0.0, 0.6862745098039216, 0.0],\n",
    "    71: [0.0, 0.23529411764705882, 0.5294117647058824],\n",
    "    72: [0.3137254901960784, 0.9411764705882353, 0.5882352941176471],\n",
    "    80: [0.5882352941176471, 0.9411764705882353, 1.0],\n",
    "    81: [0.0, 0.0, 1.0],\n",
    "    99: [1.0, 1.0, 0.19607843137254902],\n",
    "    252: [0.9607843137254902, 0.5882352941176471, 0.39215686274509803],\n",
    "    256: [1.0, 0.0, 0.0],\n",
    "    253: [0.7843137254901961, 0.1568627450980392, 1.0],\n",
    "    254: [0.11764705882352941, 0.11764705882352941, 1.0],\n",
    "    255: [0.35294117647058826, 0.11764705882352941, 0.5882352941176471],\n",
    "    257: [0.9803921568627451, 0.3137254901960784, 0.39215686274509803],\n",
    "    258: [0.7058823529411765, 0.11764705882352941, 0.3137254901960784],\n",
    "    259: [1.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "reverse_color_dict = {tuple(v): k for k, v in color_dict_normalized.items()}\n",
    "\n",
    "\n",
    "def color_pcd_kitti(pcd,labels): \n",
    "        unique_labels = np.unique(labels)\n",
    "        pcd_colors = np.zeros_like(np.asarray(pcd.points))\n",
    "        for i in unique_labels:\n",
    "                idcs = np.where(labels == i)\n",
    "                idcs = idcs[0]\n",
    "                pcd_colors[idcs] = np.array(color_dict_normalized[int(i)])\n",
    "        \n",
    "        pcd.colors = o3d.utility.Vector3dVector(pcd_colors)\n",
    "        return pcd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_instance(sem_classes,labels_instances,labels_sem):\n",
    "    num_inst = len(np.unique(labels_instances)) - 1\n",
    "\n",
    "    num_sem_inst = len(np.unique(labels_sem[np.isin(labels_sem, sem_classes)]))\n",
    "\n",
    "    num_whole_inst = num_inst + num_sem_inst\n",
    "    return num_whole_inst, labels_sem\n",
    "    \n",
    "def get_semantic_map(inst_labels, sem_labels, sem_classes=[49, 50, 51, 70, 72]):\n",
    "\n",
    "    semantic_map = {}\n",
    "    total_merges = 0\n",
    "\n",
    "    inst_labels = inst_labels[np.isin(sem_labels, sem_classes)]\n",
    "    sem_labels = sem_labels[np.isin(sem_labels, sem_classes)]\n",
    "    for inst in np.unique(inst_labels)[1:]:\n",
    "        intersect, area = np.unique(sem_labels[inst_labels == inst], return_counts=True)\n",
    "        main_sem_label = intersect[np.argmax(area)]\n",
    "        semantic_map[inst] = main_sem_label\n",
    "        to_merge_sem_labels = intersect[intersect != main_sem_label]\n",
    "        num_merges = len(to_merge_sem_labels)\n",
    "        total_merges += num_merges\n",
    "    return semantic_map\n",
    "    \n",
    "def get_num_splits(intersection, sem_classes, t_area, t_score, t_ratio):\n",
    "    result = {\n",
    "        \"inst\": [],\n",
    "        \"area\": [],\n",
    "        \"score\": [],\n",
    "        \"sem_labels\": [],\n",
    "        \"num_splits\": [],\n",
    "        \"sem_splits\": [],\n",
    "        \"main_sem_label\": [],\n",
    "        \"other_splits\": [],\n",
    "        \"valid_sem_label\": [],\n",
    "        \"sem-sem_splits\": [],\n",
    "    }\n",
    "    for idx, inst in enumerate(intersection[\"inst\"]):\n",
    "        # print(f\"inst: {inst}, area: {intersection['area'][inst]}, score: {intersection['score'][inst]}, sem_labels: {intersection['sem_labels'][inst]}, num_intersection: {intersection['num_intersection'][inst]}\")\n",
    "        area = np.sum(intersection[\"area\"][idx])\n",
    "        score = intersection[\"score\"][idx]\n",
    "        sem_label = intersection[\"sem_labels\"][idx]\n",
    "        if area > t_area and score < t_score and inst != 0:\n",
    "            intersect_area = intersection[\"area\"][idx]\n",
    "            num_splits = np.count_nonzero(intersect_area / area > t_ratio) - 1\n",
    "            valid_sem_label = sem_label[intersect_area / area > t_ratio]\n",
    "            sem_sem_splits = np.count_nonzero(np.isin(valid_sem_label, sem_classes)) - 1\n",
    "            other_splits = num_splits - sem_sem_splits\n",
    "            result[\"inst\"].append(inst)\n",
    "            result[\"area\"].append(area)\n",
    "            result[\"score\"].append(score)\n",
    "            result[\"sem_labels\"].append(sem_label)\n",
    "            result[\"num_splits\"].append(num_splits)\n",
    "            result[\"sem_splits\"].append(sem_label[intersect_area / area > t_ratio])\n",
    "            result[\"main_sem_label\"].append(sem_label[np.argmax(intersect_area)])\n",
    "            result[\"other_splits\"].append(other_splits)\n",
    "            result[\"valid_sem_label\"].append(valid_sem_label)\n",
    "            result[\"sem-sem_splits\"].append(sem_sem_splits)\n",
    "        num_splits = np.sum(result[\"num_splits\"])\n",
    "        sem2sem_splits = np.sum(result[\"sem-sem_splits\"])\n",
    "        # print(f\"inst: {inst}, area: {area}, score: {score}, sem_labels: {intersection['sem_labels'][idx]}, num_intersection: {intersection['num_intersection'][idx]}, num_splits: {num_splits}\")\n",
    "    return result, (num_splits, sem2sem_splits)\n",
    "    \n",
    "\n",
    "def create_sub_instances(inst_labels, sem_labels, result):\n",
    "    new_inst_labels = inst_labels.copy()\n",
    "    last_inst_id = np.max(inst_labels)\n",
    "    for inst in np.unique(inst_labels):\n",
    "\n",
    "        if inst in result[\"inst\"]:\n",
    "            idx = np.where(result[\"inst\"] == inst)[0][0]\n",
    "            sem_splits = result[\"sem_splits\"][idx]\n",
    "            main_sem_label = result[\"main_sem_label\"][idx]\n",
    "            to_split = sem_splits[sem_splits != main_sem_label]\n",
    "            for split in to_split:\n",
    "                last_inst_id += 1\n",
    "                # print(f\"Splitting instance {inst} with sem label {split} into {last_inst_id}\")\n",
    "                new_inst_labels[\n",
    "                    np.logical_and(inst_labels == inst, sem_labels == split)\n",
    "                ] = last_inst_id\n",
    "    return new_inst_labels\n",
    "    \n",
    "def get_intersection(labels_sem, pred_labels, ignore_labels=[0, 1, 52, 99, 40, 48, 49]):\n",
    "    intersection = {\n",
    "        \"inst\": [],\n",
    "        \"sem_labels\": [],\n",
    "        \"area\": [],\n",
    "        \"num_intersection\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    for i in ignore_labels:\n",
    "        labels_sem[labels_sem == i] = 0\n",
    "    pred_labels = pred_labels[labels_sem != 0]\n",
    "    labels_sem = labels_sem[labels_sem != 0]\n",
    "    for label in np.unique(pred_labels):\n",
    "        intersect, area = np.unique(\n",
    "            labels_sem[pred_labels == label], return_counts=True\n",
    "        )\n",
    "        intersection[\"inst\"].append(label)\n",
    "        intersection[\"sem_labels\"].append(intersect)\n",
    "        intersection[\"area\"].append(area)\n",
    "        intersection[\"num_intersection\"].append(len(intersect))\n",
    "        max_area = np.max(area)\n",
    "        # error_area = np.sum(area[area != max_area])\n",
    "        score = max_area / np.sum(area)\n",
    "        intersection[\"score\"].append(score)\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "import hdbscan\n",
    "\n",
    "#cvc clustering setup \n",
    "#params = [2,0.4,1.5]\n",
    "\n",
    "\n",
    "def uniform_down_sample_with_indices(points, every_k_points):\n",
    "        # Create a new point cloud for the downsampled output\n",
    "\n",
    "        # List to hold the indices of the points that are kept\n",
    "        indices = []\n",
    "\n",
    "        # Iterate over the points and keep every k-th point\n",
    "        for i in range(0, points.shape[0], every_k_points):\n",
    "            indices.append(i)\n",
    "\n",
    "        return indices\n",
    "\n",
    "def downsample_chunk(points):\n",
    "        num_points_to_sample = 30000\n",
    "        every_k_points = int(\n",
    "            points.shape[0] /\n",
    "            num_points_to_sample)\n",
    "        indeces = uniform_down_sample_with_indices(\n",
    "            points, every_k_points)\n",
    "\n",
    "\n",
    "        return points[indeces]\n",
    "\n",
    "def clustering_logic(pcd_nonground_chunk, pcd_ground_chunk,\n",
    "                        eps=0.3, min_samples=10,method='hdbscan'):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on the point cloud data.\n",
    "\n",
    "    :param cur_pcd: Current point cloud for clustering.\n",
    "    :param pcd_all: All point cloud data.\n",
    "    :param eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    :param min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    :return: Cluster labels for each point in the point cloud.\n",
    "    \"\"\"\n",
    "    \n",
    "    inliers = get_statistical_inlier_indices(pcd_ground_chunk)\n",
    "    ground_inliers = get_subpcd(pcd_ground_chunk, inliers)\n",
    "    mean_hight = np.mean(np.asarray(ground_inliers.points)[:,2])\n",
    "    in_idcs = np.where(np.asarray(ground_inliers.points)[:,2] < (mean_hight + 0.2))[0]\n",
    "    cut_hight = get_subpcd(ground_inliers, in_idcs)\n",
    "    cut_hight.paint_uniform_color([0, 0, 0])\n",
    "    \n",
    "    #in_idcs = np.where(np.asarray(pcd_nonground_chunk.points)[:,2] > (mean_hight + 0.05))[0]\n",
    "    #pcd_nonground_corrected = get_subpcd(pcd_nonground_chunk, in_idcs)\n",
    "    pcd_nonground_corrected = pcd_nonground_chunk\n",
    "    \n",
    "    pcd_nonground_downsampled = o3d.geometry.PointCloud()\n",
    "    pts_downsampled = downsample_chunk(np.asarray(pcd_nonground_corrected.points))\n",
    "    pcd_nonground_downsampled.points = o3d.utility.Vector3dVector(pts_downsampled)\n",
    "    \n",
    "    #clustering = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    #clustering = HDBSCAN(min_cluster_size=10).fit(pts_downsampled)\n",
    "    if method == 'hdbscan': \n",
    "        clustering = hdbscan.HDBSCAN(algorithm='best', alpha=1., approx_min_span_tree=True,\n",
    "                                    gen_min_span_tree=True, leaf_size=100,\n",
    "                                    metric='euclidean', min_cluster_size=10, min_samples=None\n",
    "                                )\n",
    "        clustering.fit(pts_downsampled)\n",
    "        \n",
    "        labels_not_road = clustering.labels_\n",
    "\n",
    "        #labels_not_road = np.asarray(cluster_indices) \n",
    "        \n",
    "    colors_gen = generate_random_colors(5000)\n",
    "    \n",
    "    labels_not_road = labels_not_road + 1\n",
    "    # Reproject cluster labels to the original point cloud size\n",
    "    cluster_labels = np.ones((len(pcd_nonground_corrected.points), 1)) * -1\n",
    "    labels_non_ground = kDTree_1NN_feature_reprojection(cluster_labels, pcd_nonground_corrected, labels_not_road.reshape(-1,1), pcd_nonground_downsampled )\n",
    "    colors = np.zeros((labels_non_ground.shape[0],3))\n",
    "    unique_labels = list(np.unique(labels_non_ground))\n",
    "    \n",
    "    for j in unique_labels:\n",
    "            cur_idcs = np.where(labels_non_ground == j)[0]\n",
    "            if j == 0 : \n",
    "                colors[cur_idcs] = np.array([0,0,0])\n",
    "            else : \n",
    "                colors[cur_idcs] = np.array(colors_gen[unique_labels.index(j)])\n",
    "        \n",
    "    pcd_nonground_corrected.colors = o3d.utility.Vector3dVector(colors / 255.)\n",
    "    \n",
    "    #o3d.visualization.draw_geometries([pcd_nonground_corrected])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pcd_nonground_corrected, cut_hight, inliers, in_idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "alpha = 1.0\n",
    "theta = 0.5\n",
    "colors = generate_random_colors_map(600)\n",
    "beta = 0.0\n",
    "gamma = 0.1\n",
    "proximity_threshold = 1.0\n",
    "tarl_norm = False\n",
    "ncuts_thresholds = [0.0025,0.005,0.01,0.02]\n",
    "        \n",
    "out_name = 'tarl_dino_spatial' + str(SEQUENCE_NUM) \n",
    "# Generate 30 different colors\n",
    " \n",
    "\n",
    "COLORS = generate_random_colors(400)\n",
    "COLORS = [(col[0]/255.,col[1]/255.,col[2]/255.) for col in COLORS]\n",
    "\n",
    "\n",
    "out_kitti = out_chunks + 'out_kitti' + str(SEQUENCE_NUM) + '/'\n",
    "if os.path.exists(out_kitti) == True : \n",
    "        shutil.rmtree(out_kitti)\n",
    "\n",
    "os.makedirs(out_kitti)\n",
    "        \n",
    "out_kitti_instance = out_chunks + 'out_kitti_instance' + str(SEQUENCE_NUM) + '/'\n",
    "\n",
    "\n",
    "if os.path.exists(out_kitti_instance) == False : \n",
    "        os.makedirs(out_kitti_instance)\n",
    "\n",
    "\n",
    "out_kitti_semantic = out_chunks + 'out_kitti_semantic' + str(SEQUENCE_NUM) + '/'\n",
    "\n",
    "if os.path.exists(out_kitti_semantic) == False : \n",
    "        os.makedirs(out_kitti_semantic)\n",
    "\n",
    "\n",
    "limit = -1 ##use this for experiments to run limit chunks numberss\n",
    "\n",
    "sem_classes = [49, 50, 51, 70, 72]\n",
    "ignore_labels = [0, 1, 52, 99, 40, 48, 49]\n",
    "\n",
    "\n",
    "patchwise_indices = indices_per_patch(T_pcd, center_positions, positions, first_position, sampled_indices_global, chunk_size)\n",
    "out_data = []\n",
    "semantics = np.hstack((kitti_labels_orig['seg_nonground'].reshape(-1,),kitti_labels_orig['seg_ground'].reshape(-1,)))\n",
    "\n",
    "instances = np.hstack((kitti_labels_orig['instance_nonground'].reshape(-1,),kitti_labels_orig['instance_ground'].reshape(-1,)))\n",
    "                \n",
    "merge_pcd = o3d.geometry.PointCloud()    \n",
    "\n",
    "print(\"total sequence number\",len(center_ids))\n",
    "for ncuts_threshold in ncuts_thresholds: \n",
    "        print(\"using threshold\",ncuts_threshold)\n",
    "        for sequence in tqdm(range(1,2)):\n",
    "                        '''\n",
    "                        merged_chunk,file_name, pcd_chunk, pcd_chunk_ground,inliers, inliers_ground = ncuts_chunk(dataset,list(indices),pcd_nonground_chunks,pcd_ground_chunks,\n",
    "                                pcd_nonground_chunks_major_downsampling,\n",
    "                                pcd_nonground_minor,T_pcd,center_positions,center_ids,\n",
    "                                positions,first_position,list(sampled_indices_global),\n",
    "                                chunk_size=chunk_size,major_voxel_size=major_voxel_size,\n",
    "                                alpha=alpha,beta=beta,gamma=gamma,theta=theta,\n",
    "                                proximity_threshold=proximity_threshold,\n",
    "                                out_folder=out_folder_ncuts,ground_mode=False,sequence=sequence,\n",
    "                                patchwise_indices=patchwise_indices,ncuts_threshold=ncuts_threshold,obb=obbs[sequence])\n",
    "                        '''\n",
    "                        pcd_chunk, pcd_chunk_ground, inliers, inliers_ground = clustering_logic(pcd_nonground_chunks[sequence],pcd_ground_chunks[sequence],\n",
    "                        eps=0.4, min_samples=10,method='hdbscan')\n",
    "        \n",
    "                        \n",
    "                        #o3d.visualization.draw_geometries([merged_chunk])\n",
    "                        \n",
    "                        seg_ground = kitti_labels['ground']['semantic'][sequence][inliers][inliers_ground]\n",
    "                        inst_ground = kitti_labels['ground']['instance'][sequence][inliers][inliers_ground]\n",
    "                        \n",
    "                        file_name = str(center_ids[sequence]).zfill(6) + '.pcd'\n",
    "        \n",
    "                        kitti_chunk = color_pcd_by_labels(pcd_chunk,kitti_labels['nonground']['semantic'][sequence].reshape(-1,),\n",
    "                                                colors=COLORS,gt_labels=semantics,semantics=True\n",
    "                                                )\n",
    "                        \n",
    "                        kitti_chunk_instance = color_pcd_by_labels(pcd_chunk,kitti_labels['nonground']['instance'][sequence].reshape(-1,),\n",
    "                                                colors=colors,gt_labels=instances)\n",
    "                                                \n",
    "                        kitti_chunk_instance_ground = color_pcd_by_labels(pcd_chunk_ground,inst_ground.reshape(-1,),\n",
    "                                                colors=colors,gt_labels=instances)\n",
    "                                                \n",
    "                        kitti_chunk_semantic_ground = color_pcd_by_labels(pcd_chunk_ground,seg_ground.reshape(-1,),\n",
    "                                                colors=COLORS,gt_labels=semantics,semantics=True)\n",
    "                        \n",
    "                        #o3d.visualization.draw_geometries([kitti_chunk_instance + kitti_chunk_instance_ground])\n",
    "                        #o3d.visualization.draw_geometries([pcd_chunk + pcd_chunk_ground])\n",
    "                        #semantic_labels = np.hstack(( kitti_labels['nonground']['semantic'][sequence].reshape(-1,),seg_ground.reshape(-1,)))\n",
    "                        #np.savez(out_kitti_semantic + file_name.split('.')[0] + '.npz',labels=semantic_labels)\n",
    "                        semantics_non_ground = color_pcd_kitti(copy.deepcopy(pcd_chunk),kitti_labels['nonground']['semantic'][sequence].reshape(-1,))\n",
    "                        semantics_ground = color_pcd_kitti(copy.deepcopy(pcd_chunk_ground),seg_ground.reshape(-1,))\n",
    "                        kitti_semantics = np.hstack((kitti_labels['nonground']['semantic'][sequence].reshape(-1,),seg_ground.reshape(-1,)))\n",
    "                        \n",
    "                        kitti_instance = kitti_chunk_instance + kitti_chunk_instance_ground\n",
    "                        pcd_merged = pcd_chunk + pcd_chunk_ground\n",
    "                        \n",
    "                        unique_colors, labels_kitti = np.unique(np.asarray(kitti_instance.colors),axis=0, return_inverse=True)\n",
    "                        unique_colors, labels_ncuts = np.unique(np.asarray(pcd_merged.colors),axis=0, return_inverse=True)\n",
    "                        \n",
    "                        new_ncuts_labels = remove_semantics(labels_kitti,copy.deepcopy(labels_ncuts))\n",
    "                        metrics_ncuts = Metrics(name='ncuts with threshold ' + str(ncuts_threshold) )\n",
    "                        metrics_ncuts.update_stats(labels_ncuts,new_ncuts_labels,labels_kitti)\n",
    "                        \n",
    "                        \n",
    "                        ##semantic stuff \n",
    "                        num_instance, sem_labels = calc_num_instance(sem_classes,labels_kitti,kitti_semantics)\n",
    "                        \n",
    "                        intersection = get_intersection(sem_labels, labels_ncuts)\n",
    "                        print('intersection')\n",
    "                        print(intersection)\n",
    "                        result, (num_splits, sem2sem_splits) = get_num_splits(intersection, sem_classes, 200, 0.9, 0.02)\n",
    "                        new_labels = create_sub_instances(labels_ncuts, sem_labels, result)\n",
    "                        semantic_map = get_semantic_map(new_labels, sem_labels)\n",
    "                        value_counts = Counter(semantic_map.values())\n",
    "                        num_merges = np.sum(list(value_counts.values())) - len(value_counts)\n",
    "                        \n",
    "                        print('number of splits',num_splits)\n",
    "                        print('sem sem splits', sem2sem_splits)\n",
    "                        print('num merges', num_merges)\n",
    "                        print('---------------------')\n",
    "                        \n",
    "                        #o3d.visualization.draw_geometries([kitti_instance])\n",
    "                        #o3d.visualization.draw_geometries([semantics_ground + semantics_non_ground])\n",
    "                        o3d.visualization.draw_geometries([pcd_merged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
