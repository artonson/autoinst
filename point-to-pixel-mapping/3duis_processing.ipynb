{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[(0.267004, 0.004874, 0.329415), (0.277018, 0.050344, 0.375715), (0.282656, 0.100196, 0.42216), (0.28229, 0.145912, 0.46151), (0.276194, 0.190074, 0.493001), (0.265145, 0.232956, 0.516599), (0.252194, 0.269783, 0.531579), (0.235526, 0.309527, 0.542944), (0.21813, 0.347432, 0.550038), (0.201239, 0.38367, 0.554294), (0.185556, 0.41857, 0.556753), (0.171176, 0.45253, 0.557965), (0.159194, 0.482237, 0.558073), (0.14618, 0.515413, 0.556823), (0.133743, 0.548535, 0.553541), (0.123463, 0.581687, 0.547445), (0.119483, 0.614817, 0.537692), (0.128087, 0.647749, 0.523491), (0.150148, 0.676631, 0.506589), (0.19109, 0.708366, 0.482284), (0.24607, 0.73891, 0.452024), (0.311925, 0.767822, 0.415586), (0.386433, 0.794644, 0.372886), (0.468053, 0.818921, 0.323998), (0.545524, 0.838039, 0.275626), (0.636902, 0.856542, 0.21662), (0.730889, 0.871916, 0.156029), (0.82494, 0.88472, 0.106217), (0.916242, 0.896091, 0.100717), (0.993248, 0.906157, 0.143936)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cedric/anaconda3/envs/torch/lib/python3.9/site-packages/MinkowskiEngine-0.5.4-py3.9-linux-x86_64.egg/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PatchWorkpp::PatchWorkpp() - INITIALIZATION COMPLETE\n",
      "PatchWorkpp::PatchWorkpp() - INITIALIZATION COMPLETE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "%matplotlib inline \n",
    "\n",
    "src_path = os.path.abspath(\"../..\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "%load_ext autoreload\n",
    "from dataset.kitti_odometry_dataset import KittiOdometryDataset, KittiOdometryDatasetConfig\n",
    "from dataset.filters.filter_list import FilterList\n",
    "from dataset.filters.kitti_gt_mo_filter import KittiGTMovingObjectFilter\n",
    "from dataset.filters.range_filter import RangeFilter\n",
    "from dataset.filters.apply_pose import ApplyPose\n",
    "from tqdm import tqdm \n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from normalized_cut import normalized_cut\n",
    "from ncuts_utils import ncuts_chunk,kDTree_1NN_feature_reprojection_colors, get_merge_pcds\n",
    "from dataset_utils import * \n",
    "from point_cloud_utils import get_pcd, transform_pcd, kDTree_1NN_feature_reprojection, remove_isolated_points, get_subpcd, get_statistical_inlier_indices, merge_chunks_unite_instances\n",
    "from aggregate_pointcloud import aggregate_pointcloud\n",
    "from visualization_utils import generate_random_colors, color_pcd_by_labels,generate_random_colors_map\n",
    "from sam_label_distace import sam_label_distance\n",
    "from chunk_generation import subsample_positions, chunks_from_pointcloud, indices_per_patch, tarl_features_per_patch, image_based_features_per_patch, dinov2_mean, get_indices_feature_reprojection\n",
    "from metrics.metrics_class import Metrics\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from chunk_generation import get_indices_feature_reprojection\n",
    "from chunk_generation import subsample_positions, chunks_from_pointcloud, indices_per_patch, tarl_features_per_patch, image_based_features_per_patch, dinov2_mean, get_indices_feature_reprojection\n",
    "from point_cloud_utils import get_pcd, transform_pcd, remove_isolated_points, get_subpcd, get_statistical_inlier_indices, merge_chunks_unite_instances, kDTree_1NN_feature_reprojection\n",
    "\n",
    "\n",
    "from utils.UIS.utils import *\n",
    "from utils.UIS.minkunet import *\n",
    "from utils.UIS.collations import *\n",
    "from utils.UIS.corr_utils import *\n",
    "from utils.UIS.pcd_preprocess import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the dataset depending on kitti sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('/media/cedric/Datasets1/semantic_kitti/')\n",
    "end_inds = {0:4541,1:1100,2:4661,3:800,4:271,5:2761,6:1101,7:1100,8:4071,9:1591,10:1201}\n",
    "SEQUENCE_NUM = 3\n",
    "\n",
    "ind_start = 0\n",
    "ind_end = end_inds[SEQUENCE_NUM]\n",
    "\n",
    "minor_voxel_size = 0.05\n",
    "major_voxel_size = 0.35\n",
    "chunk_size = np.array([25, 25, 25]) #meters\n",
    "overlap = 15 #meters\n",
    "ground_segmentation_method = 'patchwork' \n",
    "NCUT_ground = False \n",
    "\n",
    "out_chunks = 'pcd_preprocessed/output_chunks/'\n",
    "out_folder_ncuts = 'test_data/'\n",
    "if os.path.exists(out_folder_ncuts):\n",
    "        shutil.rmtree(out_folder_ncuts)\n",
    "os.makedirs(out_folder_ncuts)\n",
    "\n",
    "dataset = create_kitti_odometry_dataset(DATASET_PATH,SEQUENCE_NUM,ncuts_mode=True)\n",
    "\n",
    "out_folder = 'pcd_preprocessed/'\n",
    "if os.path.exists(out_folder) == False : \n",
    "        os.makedirs(out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can be ignored after first run as outputs are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data if already stored \n",
    "\n",
    "if os.path.exists(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd') == False:\n",
    "        pcd_ground_minor, pcd_nonground_minor,\\\n",
    "                all_poses, T_pcd, first_position,kitti_labels = load_and_downsample_point_clouds(out_folder,SEQUENCE_NUM,minor_voxel_size,\\\n",
    "                                                                        ground_mode=ground_segmentation_method)\n",
    "        #o3d.visualization.draw_geometries([color_pcd_by_labels(pcd_nonground_minor,kitti_labels['seg_nonground'])])\n",
    "        o3d.io.write_point_cloud(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd', pcd_ground_minor, write_ascii=False, compressed=False, print_progress=False)\n",
    "        o3d.io.write_point_cloud(f'{out_folder}pcd_nonground_minor{SEQUENCE_NUM}_0.pcd', pcd_nonground_minor, write_ascii=False, compressed=False, print_progress=False)\n",
    "        np.savez(f'{out_folder}kitti_labels_preprocessed{SEQUENCE_NUM}_0.npz',\n",
    "                                                instance_nonground=kitti_labels['instance_nonground'],\n",
    "                                                instance_ground=kitti_labels['instance_ground'],\n",
    "                                                seg_ground = kitti_labels['seg_ground'],\n",
    "                                                seg_nonground=kitti_labels['seg_nonground']\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointCloud with 11891064 points.\n"
     ]
    }
   ],
   "source": [
    "pcd_ground_minor = o3d.io.read_point_cloud(f'{out_folder}pcd_ground_minor{SEQUENCE_NUM}_0.pcd')\n",
    "pcd_nonground_minor = o3d.io.read_point_cloud(f'{out_folder}pcd_nonground_minor{SEQUENCE_NUM}_0.pcd')\n",
    "print(pcd_ground_minor)\n",
    "kitti_labels_orig = {}\n",
    "with np.load(f'{out_folder}kitti_labels_preprocessed{SEQUENCE_NUM}_0.npz') as data :\n",
    "        kitti_labels_orig['instance_ground'] = data['instance_ground']\n",
    "        kitti_labels_orig['instance_nonground'] = data['instance_nonground']\n",
    "        kitti_labels_orig['seg_nonground'] = data['seg_nonground']\n",
    "        kitti_labels_orig['seg_ground'] = data['seg_ground']\n",
    "\n",
    "        \n",
    "\n",
    "with np.load(f'{out_folder}all_poses_{SEQUENCE_NUM}_0.npz') as data:\n",
    "        all_poses = data['all_poses']\n",
    "        T_pcd = data['T_pcd']\n",
    "        first_position = T_pcd[:3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npcd_new = o3d.geometry.PointCloud()\\npts_num = 1000000\\npcd_new.points = o3d.utility.Vector3dVector(np.asarray(pcd_nonground_minor.points)[:pts_num])\\n\\nmap_labelled = color_pcd_by_labels(pcd_new,                kitti_labels['panoptic_nonground'][:pts_num].reshape(-1,1))\\n\\no3d.visualization.draw_geometries([map_labelled])\\n#o3d.io.write_point_cloud('labelled_map07.pcd',map_labelled)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pcd_new = o3d.geometry.PointCloud()\n",
    "pts_num = 1000000\n",
    "pcd_new.points = o3d.utility.Vector3dVector(np.asarray(pcd_nonground_minor.points)[:pts_num])\n",
    "\n",
    "map_labelled = color_pcd_by_labels(pcd_new,\\\n",
    "                kitti_labels['panoptic_nonground'][:pts_num].reshape(-1,1))\n",
    "\n",
    "o3d.visualization.draw_geometries([map_labelled])\n",
    "#o3d.io.write_point_cloud('labelled_map07.pcd',map_labelled)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we subsample the poses based on a voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{out_folder}subsampled_data{str(SEQUENCE_NUM)}_0.npz') == False : \n",
    "\tprint(f'{out_folder}subsampled_data{str(SEQUENCE_NUM)}_0.npz')\n",
    "\tposes, positions, \\\n",
    "\tsampled_indices_local, sampled_indices_global = subsample_and_extract_positions(all_poses,ind_start=ind_start,sequence_num=SEQUENCE_NUM,out_folder=out_folder)\n",
    "\n",
    "\n",
    "with np.load(f'{out_folder}subsampled_data{SEQUENCE_NUM}_0.npz') as data:\n",
    "\tposes=data['poses']\n",
    "\tpositions=data['positions']\n",
    "\tsampled_indices_local = data['sampled_indices_local']\n",
    "\tsampled_indices_global=data['sampled_indices_global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the point cloud into chunks based on a tbd chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled from (347268, 3) to (6179, 3) points (non-ground)\n",
      "Downsampled from (238409, 3) to (4941, 3) points (ground)\n",
      "Downsampled from (405613, 3) to (6493, 3) points (non-ground)\n",
      "Downsampled from (271374, 3) to (5029, 3) points (ground)\n",
      "Downsampled from (376960, 3) to (6157, 3) points (non-ground)\n",
      "Downsampled from (346533, 3) to (5820, 3) points (ground)\n",
      "Downsampled from (378592, 3) to (6043, 3) points (non-ground)\n",
      "Downsampled from (452272, 3) to (6908, 3) points (ground)\n",
      "Downsampled from (213777, 3) to (3189, 3) points (non-ground)\n",
      "Downsampled from (497621, 3) to (6935, 3) points (ground)\n",
      "Downsampled from (244237, 3) to (3589, 3) points (non-ground)\n",
      "Downsampled from (474830, 3) to (6738, 3) points (ground)\n",
      "Downsampled from (316651, 3) to (4900, 3) points (non-ground)\n",
      "Downsampled from (392208, 3) to (6399, 3) points (ground)\n",
      "Downsampled from (275619, 3) to (4188, 3) points (non-ground)\n",
      "Downsampled from (371678, 3) to (6053, 3) points (ground)\n",
      "Downsampled from (271919, 3) to (4328, 3) points (non-ground)\n",
      "Downsampled from (389575, 3) to (6043, 3) points (ground)\n",
      "Downsampled from (329319, 3) to (5337, 3) points (non-ground)\n",
      "Downsampled from (395304, 3) to (6117, 3) points (ground)\n",
      "Downsampled from (434874, 3) to (7617, 3) points (non-ground)\n",
      "Downsampled from (409050, 3) to (6236, 3) points (ground)\n",
      "Downsampled from (494864, 3) to (8786, 3) points (non-ground)\n",
      "Downsampled from (440951, 3) to (6575, 3) points (ground)\n",
      "Downsampled from (506192, 3) to (8615, 3) points (non-ground)\n",
      "Downsampled from (428335, 3) to (6197, 3) points (ground)\n",
      "Downsampled from (404370, 3) to (6462, 3) points (non-ground)\n",
      "Downsampled from (419737, 3) to (5886, 3) points (ground)\n",
      "Downsampled from (365793, 3) to (6289, 3) points (non-ground)\n",
      "Downsampled from (413241, 3) to (6033, 3) points (ground)\n",
      "Downsampled from (273241, 3) to (4356, 3) points (non-ground)\n",
      "Downsampled from (421816, 3) to (6019, 3) points (ground)\n",
      "Downsampled from (309809, 3) to (4850, 3) points (non-ground)\n",
      "Downsampled from (432158, 3) to (6049, 3) points (ground)\n",
      "Downsampled from (326409, 3) to (5413, 3) points (non-ground)\n",
      "Downsampled from (420227, 3) to (5955, 3) points (ground)\n",
      "Downsampled from (374570, 3) to (7104, 3) points (non-ground)\n",
      "Downsampled from (399573, 3) to (5795, 3) points (ground)\n",
      "Downsampled from (427959, 3) to (8015, 3) points (non-ground)\n",
      "Downsampled from (395006, 3) to (5635, 3) points (ground)\n",
      "Downsampled from (411448, 3) to (8266, 3) points (non-ground)\n",
      "Downsampled from (419548, 3) to (6308, 3) points (ground)\n",
      "Downsampled from (411640, 3) to (8313, 3) points (non-ground)\n",
      "Downsampled from (417475, 3) to (6174, 3) points (ground)\n",
      "Downsampled from (351530, 3) to (6928, 3) points (non-ground)\n",
      "Downsampled from (433993, 3) to (6133, 3) points (ground)\n",
      "Downsampled from (335139, 3) to (5668, 3) points (non-ground)\n",
      "Downsampled from (463342, 3) to (6773, 3) points (ground)\n",
      "Downsampled from (358697, 3) to (6196, 3) points (non-ground)\n",
      "Downsampled from (424106, 3) to (6240, 3) points (ground)\n",
      "Downsampled from (313890, 3) to (5327, 3) points (non-ground)\n",
      "Downsampled from (452732, 3) to (6309, 3) points (ground)\n",
      "Downsampled from (237479, 3) to (4675, 3) points (non-ground)\n",
      "Downsampled from (439565, 3) to (6481, 3) points (ground)\n",
      "Downsampled from (308673, 3) to (5824, 3) points (non-ground)\n",
      "Downsampled from (367565, 3) to (5713, 3) points (ground)\n",
      "Downsampled from (290530, 3) to (5474, 3) points (non-ground)\n",
      "Downsampled from (352431, 3) to (5706, 3) points (ground)\n",
      "Downsampled from (315329, 3) to (5729, 3) points (non-ground)\n",
      "Downsampled from (370001, 3) to (5770, 3) points (ground)\n",
      "Downsampled from (329512, 3) to (5563, 3) points (non-ground)\n",
      "Downsampled from (393310, 3) to (6032, 3) points (ground)\n",
      "Downsampled from (424590, 3) to (7254, 3) points (non-ground)\n",
      "Downsampled from (407664, 3) to (6577, 3) points (ground)\n",
      "Downsampled from (411633, 3) to (6984, 3) points (non-ground)\n",
      "Downsampled from (428493, 3) to (6737, 3) points (ground)\n",
      "Downsampled from (341658, 3) to (5511, 3) points (non-ground)\n",
      "Downsampled from (423208, 3) to (6345, 3) points (ground)\n",
      "Downsampled from (290812, 3) to (4334, 3) points (non-ground)\n",
      "Downsampled from (394945, 3) to (5497, 3) points (ground)\n",
      "Downsampled from (360932, 3) to (4827, 3) points (non-ground)\n",
      "Downsampled from (421445, 3) to (5688, 3) points (ground)\n",
      "Downsampled from (432985, 3) to (6302, 3) points (non-ground)\n",
      "Downsampled from (453672, 3) to (6150, 3) points (ground)\n",
      "Downsampled from (415994, 3) to (6826, 3) points (non-ground)\n",
      "Downsampled from (474288, 3) to (6852, 3) points (ground)\n",
      "Downsampled from (446293, 3) to (7717, 3) points (non-ground)\n",
      "Downsampled from (463875, 3) to (6884, 3) points (ground)\n",
      "Downsampled from (374518, 3) to (6926, 3) points (non-ground)\n",
      "Downsampled from (470591, 3) to (6955, 3) points (ground)\n",
      "Downsampled from (315267, 3) to (6113, 3) points (non-ground)\n",
      "Downsampled from (465647, 3) to (6835, 3) points (ground)\n",
      "Downsampled from (347088, 3) to (6692, 3) points (non-ground)\n",
      "Downsampled from (452459, 3) to (6804, 3) points (ground)\n",
      "Downsampled from (331140, 3) to (6635, 3) points (non-ground)\n",
      "Downsampled from (466165, 3) to (6847, 3) points (ground)\n",
      "Downsampled from (260433, 3) to (5640, 3) points (non-ground)\n",
      "Downsampled from (508403, 3) to (7159, 3) points (ground)\n",
      "Downsampled from (121076, 3) to (3769, 3) points (non-ground)\n",
      "Downsampled from (538788, 3) to (7586, 3) points (ground)\n",
      "Downsampled from (96833, 3) to (3037, 3) points (non-ground)\n",
      "Downsampled from (533200, 3) to (7572, 3) points (ground)\n",
      "Downsampled from (135172, 3) to (3429, 3) points (non-ground)\n",
      "Downsampled from (535016, 3) to (7509, 3) points (ground)\n",
      "Downsampled from (177995, 3) to (3650, 3) points (non-ground)\n",
      "Downsampled from (515777, 3) to (7333, 3) points (ground)\n",
      "Downsampled from (224659, 3) to (3874, 3) points (non-ground)\n",
      "Downsampled from (483997, 3) to (7043, 3) points (ground)\n",
      "Downsampled from (294243, 3) to (4594, 3) points (non-ground)\n",
      "Downsampled from (448835, 3) to (6767, 3) points (ground)\n",
      "Downsampled from (301487, 3) to (4372, 3) points (non-ground)\n",
      "Downsampled from (432641, 3) to (6174, 3) points (ground)\n",
      "Downsampled from (272138, 3) to (3613, 3) points (non-ground)\n",
      "Downsampled from (436990, 3) to (5834, 3) points (ground)\n",
      "Downsampled from (277152, 3) to (3945, 3) points (non-ground)\n",
      "Downsampled from (427330, 3) to (5961, 3) points (ground)\n"
     ]
    }
   ],
   "source": [
    "pcd_nonground_chunks, pcd_ground_chunks,\\\n",
    "pcd_nonground_chunks_major_downsampling, pcd_ground_chunks_major_downsampling, \\\n",
    "indices,indices_ground, center_positions, \\\n",
    "center_ids, chunk_bounds, kitti_labels = chunk_and_downsample_point_clouds(pcd_nonground_minor, pcd_ground_minor, T_pcd, positions, \n",
    "                                                            first_position, sampled_indices_global, chunk_size=chunk_size, \n",
    "                                                            overlap=overlap, major_voxel_size=major_voxel_size,kitti_labels=kitti_labels_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "def set_deterministic():\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def color_pcd_by_labels(pcd, labels,colors=None,gt_labels=None):\n",
    "    \n",
    "    if colors == None : \n",
    "        colors = generate_random_colors(2000)\n",
    "    pcd_colored = copy.deepcopy(pcd)\n",
    "    pcd_colors = np.zeros(np.asarray(pcd.points).shape)\n",
    "    if gt_labels is None :\n",
    "    \tunique_labels = list(np.unique(labels)) \n",
    "    else: \n",
    "        unique_labels = list(np.unique(gt_labels))\n",
    "    \n",
    "    background_color = np.array([0,0,0])\n",
    "\n",
    "\n",
    "    #for i in range(len(pcd_colored.points)):\n",
    "    for i in unique_labels:\n",
    "        idcs = np.where(labels == i)\n",
    "        idcs = idcs[0]\n",
    "        if i == 0 : \n",
    "            pcd_colors[idcs] = background_color\n",
    "        else : \n",
    "            pcd_colors[idcs] = np.array(colors[unique_labels.index(i)])\n",
    "        \n",
    "        #if labels[i] != (-1):\n",
    "        #    pcd_colored.colors[i] = np.array(colors[labels[i]]) / 255\n",
    "    pcd_colored.colors = o3d.utility.Vector3dVector(pcd_colors/ 255)\n",
    "    return pcd_colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "import hdbscan\n",
    "import yaml \n",
    "import scipy \n",
    "\n",
    "config = 'utils/UIS/instance_seg.yaml'\n",
    "cfg = yaml.safe_load(open(config))\n",
    "params = cfg\n",
    "\n",
    "set_deterministic()\n",
    "model = MinkUNet(in_channels=4, out_channels=96).type(torch.FloatTensor)\n",
    "#checkpoint = torch.load(cfg['model']['checkpoint'], map_location=torch.device('cuda'))\n",
    "checkpoint = torch.load('utils/UIS/epoch199_model_segcontrast.pt', map_location=torch.device('cuda'))\n",
    "model.cuda()\n",
    "#model.load_state_dict(checkpoint[cfg['model']['checkpoint_key']])\n",
    "model.load_state_dict(checkpoint[cfg['model']['checkpoint_key']])\n",
    "model.dropout = nn.Identity()\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "        param.require_grads = False\n",
    "\n",
    "\n",
    "def uniform_down_sample_with_indices(points, every_k_points):\n",
    "        # Create a new point cloud for the downsampled output\n",
    "\n",
    "        # List to hold the indices of the points that are kept\n",
    "        indices = []\n",
    "\n",
    "        # Iterate over the points and keep every k-th point\n",
    "        for i in range(0, points.shape[0], every_k_points):\n",
    "            indices.append(i)\n",
    "\n",
    "        return indices\n",
    "\n",
    "def downsample_chunk(points):\n",
    "        num_points_to_sample = 30000\n",
    "        every_k_points = int(\n",
    "            points.shape[0] /\n",
    "            num_points_to_sample)\n",
    "        indeces = uniform_down_sample_with_indices(\n",
    "            points, every_k_points)\n",
    "\n",
    "\n",
    "        return points[indeces]\n",
    "        \n",
    "def segcontrast_preprocessing(p,sem_labels,resolution=0.05,num_points='inf'):\n",
    "    coord_p, feats_p, cluster_p = point_set_to_coord_feats(p, sem_labels, resolution, num_points)\n",
    "    return coord_p, feats_p, cluster_p\n",
    "\n",
    "\n",
    "def UIS3D_clustering(pcd_nonground_chunk, pcd_ground_chunk,center_id,center_position,\n",
    "                        eps=0.3, min_samples=10,tarl=False):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering on the point cloud data.\n",
    "\n",
    "    :param cur_pcd: Current point cloud for clustering.\n",
    "    :param pcd_all: All point cloud data.\n",
    "    :param eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "    :param min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "    :return: Cluster labels for each point in the point cloud.\n",
    "    \"\"\"\n",
    "    \n",
    "    inliers = get_statistical_inlier_indices(pcd_ground_chunk)\n",
    "    ground_inliers = get_subpcd(pcd_ground_chunk, inliers)\n",
    "    mean_hight = np.mean(np.asarray(ground_inliers.points)[:,2])\n",
    "    in_idcs = np.where(np.asarray(ground_inliers.points)[:,2] < (mean_hight + 0.2))[0]\n",
    "    cut_hight = get_subpcd(ground_inliers, in_idcs)\n",
    "    cut_hight.paint_uniform_color([0, 0, 0])\n",
    "    \n",
    "    in_idcs = None\n",
    "    \n",
    "    #in_idcs = np.where(np.asarray(pcd_nonground_chunk.points)[:,2] > (mean_hight + 0.05))[0]\n",
    "    #pcd_nonground_corrected = get_subpcd(pcd_nonground_chunk, in_idcs)\n",
    "    pcd_nonground_corrected = pcd_nonground_chunk\n",
    "    \n",
    "    merge_orig = pcd_nonground_corrected + cut_hight\n",
    "    \n",
    "    pcd_nonground_downsampled = o3d.geometry.PointCloud()\n",
    "    pts_downsampled = downsample_chunk(np.asarray(pcd_nonground_corrected.points))\n",
    "    pcd_nonground_downsampled.points = o3d.utility.Vector3dVector(pts_downsampled)\n",
    "    \n",
    "    ground_downsampled = o3d.geometry.PointCloud()\n",
    "    pts_downsampled_ground = downsample_chunk(np.asarray(cut_hight.points))\n",
    "    ground_downsampled.points = o3d.utility.Vector3dVector(pts_downsampled_ground)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #clustering = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    #clustering = HDBSCAN(min_cluster_size=10).fit(pts_downsampled)\n",
    "    clustering = hdbscan.HDBSCAN(algorithm='best', alpha=1., approx_min_span_tree=True,\n",
    "                                gen_min_span_tree=True, leaf_size=100,\n",
    "                                metric='euclidean', min_cluster_size=10, min_samples=None\n",
    "                            )\n",
    "    clustering.fit(pts_downsampled)\n",
    "    \n",
    "    \n",
    "    merged_chunk = pcd_nonground_downsampled + ground_downsampled\n",
    "    \n",
    "    labels_nonground = clustering.labels_.reshape(-1,1) + 2\n",
    "    points = np.asarray(merged_chunk.points)\n",
    "    labels = np.ones((points.shape[0], 1)) * -1\n",
    "    \n",
    "    ground_labels = np.zeros(points.shape[0]) * -1\n",
    "    non_ground_size = np.asarray(pcd_nonground_downsampled.points).shape[0]\n",
    "    ground_labels[:non_ground_size] = 1\n",
    "    labels[:non_ground_size] = labels_nonground\n",
    "    pcd_cur = color_pcd_by_labels(merged_chunk,labels)\n",
    "    #o3d.visualization.draw_geometries([pcd_cur])\n",
    "    ins, num_pts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    mask = np.ones(labels.shape[0], dtype=bool)\n",
    "    mask[non_ground_size:] = False\n",
    "    points = np.concatenate((points,np.ones((points.shape[0],1))),1)\n",
    "    mean_x = points[:,0].mean() \n",
    "    mean_y = points[:,1].mean() \n",
    "    mean_z = points[:,2].mean() \n",
    "    \n",
    "    points[:,0] -= mean_x\n",
    "    points[:,1] -= mean_y\n",
    "    points[:,2] -= mean_z\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###local features from tarl : note can not be used as network saliency is needed \n",
    "    if tarl == True: \n",
    "        tarl_indices_global, _ = get_indices_feature_reprojection(sampled_indices_global, center_id, adjacent_frames=(10,10)) \n",
    "        tarl_features = tarl_features_per_patch(dataset, merged_chunk, T_pcd, center_position, tarl_indices_global, chunk_size, search_radius=minor_voxel_size/2)\n",
    "    \n",
    "    else : \n",
    "        coord_p,feats_p,cluster_p = segcontrast_preprocessing(points,labels)\n",
    "    \n",
    "    slc_full = np.zeros((points.shape[0],), dtype=int)\n",
    "    pred_ins_full = np.zeros((points.shape[0],), dtype=int)\n",
    "    for cluster in ins: \n",
    "            cls_points = np.where(cluster_p == cluster)[0]\n",
    "            \n",
    "            if cluster == 0 or len(cls_points) <= 100:\n",
    "                continue\n",
    "            # get cluster\n",
    "            cluster_center = coord_p[cls_points].mean(axis=0)\n",
    "\n",
    "            # crop a ROI around the cluster\n",
    "            window_points = crop_region(coord_p,cluster_p, cluster, 20)\n",
    "\n",
    "            # skip when ROI is empty        \n",
    "            if not np.sum(window_points):\n",
    "                continue\n",
    "\n",
    "            # get closest point to the center\n",
    "            center_dists = np.sqrt(np.sum((coord_p[window_points] - cluster_center)**2, axis=-1))\n",
    "            cluster_center = np.argmin(center_dists)\n",
    "\n",
    "            # build input only with the ROI points\n",
    "            x_forward = numpy_to_sparse_tensor(coord_p[window_points][np.newaxis, :, :], feats_p[window_points][np.newaxis, :, :])\n",
    "            \n",
    "            # forward pass ROI \n",
    "            model.eval()\n",
    "            x_forward.F.requires_grad = True\n",
    "            out = model(x_forward.sparse())\n",
    "            out = out.slice(x_forward)\n",
    "\n",
    "            # reset grads to compute saliency\n",
    "            x_forward.F.grad = None\n",
    "\n",
    "            # compute saliency for the point in the center\n",
    "            slc = get_cluster_saliency(x_forward, out, np.where(cluster_p[window_points] == cluster)[0])\n",
    "            slc_ = slc.copy()\n",
    "\n",
    "            # place the computed saliency into the full point cloud for comparison\n",
    "            slc_full[window_points] = np.maximum(slc_full[window_points], slc)\n",
    "\n",
    "            # build graph representation\n",
    "            G = build_graph(out.F.detach().cpu().numpy(),\n",
    "                            slc[:,np.newaxis],\n",
    "                            coord_p[window_points],\n",
    "                            cluster_center,\n",
    "                            np.sum(cluster_p == cluster),\n",
    "                            params,\n",
    "                            ground_labels[window_points],\n",
    "                            np.where(cluster_p[window_points] == cluster)[0],\n",
    "                        )\n",
    "            # perform graph cut\n",
    "            #G = scipy.sparse.csr_matrix(G) -> try out this line \n",
    "            ins_points = graph_cut(G)\n",
    "            # create point-wise prediction matrix\n",
    "            pred_ins = np.zeros((len(x_forward),)).astype(int)\n",
    "            if len(ins_points) != 0:\n",
    "                pred_ins[ins_points] = cluster\n",
    "            \n",
    "            # ignore assigned ground labels\n",
    "            ins_ground = ground_labels[window_points] == -1\n",
    "            pred_ins[ins_ground] = 0\n",
    "\n",
    "            pred_ins_full[window_points] = np.maximum(pred_ins_full[window_points], pred_ins)\n",
    "        \n",
    "    #pcd_cur = color_pcd_by_labels(merged_chunk,pred_ins_full)\n",
    "    #o3d.visualization.draw_geometries([pcd_cur])\n",
    "    \n",
    "    colors_gen = generate_random_colors(500)\n",
    "    \n",
    "    # Reproject cluster labels to the original point cloud size\n",
    "    cluster_labels = np.ones((len(merge_orig.points), 1)) * -1\n",
    "    labels_orig = kDTree_1NN_feature_reprojection(cluster_labels, merge_orig, pred_ins_full.reshape(-1,1), merged_chunk)\n",
    "    colors = np.zeros((labels_orig.shape[0],3))\n",
    "    unique_labels = list(np.unique(labels_orig))\n",
    "    \n",
    "    for j in unique_labels:\n",
    "            cur_idcs = np.where(labels_orig == j)[0]\n",
    "            if j == 0 : \n",
    "                colors[cur_idcs] = np.array([0,0,0])\n",
    "                \n",
    "            else : \n",
    "                colors[cur_idcs] = np.array(colors_gen[unique_labels.index(j)])\n",
    "                \n",
    "        \n",
    "    merge_orig.colors = o3d.utility.Vector3dVector(colors / 255.)\n",
    "    \n",
    "    return merge_orig\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/53 [19:07<16:34:41, 1147.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/53 [28:46<11:30:57, 812.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/53 [40:18<10:31:25, 757.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence 3\n"
     ]
    }
   ],
   "source": [
    "out_dbscan = out_chunks + 'out_3duis' +  str(SEQUENCE_NUM)  + '/'\n",
    "if os.path.exists(out_dbscan) == True : \n",
    "        shutil.rmtree(out_dbscan)\n",
    "        \n",
    "os.makedirs(out_dbscan)\n",
    "\n",
    "\n",
    "limit = -1 ##use this for experiments to run limit chunks numberss\n",
    "\n",
    "\n",
    "patchwise_indices = indices_per_patch(T_pcd, center_positions, positions, first_position, sampled_indices_global, chunk_size)\n",
    "out_data = []\n",
    "for sequence in tqdm(range(len(center_ids))):\n",
    "                print('sequence',sequence)\n",
    "                pcd_3duis = UIS3D_clustering(pcd_nonground_chunks[sequence],pcd_ground_chunks[sequence],center_ids[sequence],\n",
    "                        center_positions[sequence],\n",
    "                        eps=0.4, min_samples=10)\n",
    "                \n",
    "                #kitti_chunk_instance = color_pcd_by_labels(obstacle_chunk,kitti_labels['nonground']['instance'][sequence][in_idcs].reshape(-1,),\n",
    "                #kitti_chunk_instance = color_pcd_by_labels(obstacle_chunk,kitti_labels['nonground']['instance'][sequence].reshape(-1,),\n",
    "                #                        colors=colors,gt_labels=kitti_labels_orig['instance_nonground'])\n",
    "                \n",
    "                #o3d.visualization.draw_geometries([kitti_chunk_instance])\n",
    "                #print(kitti_chunk_instance,obstacle_chunk)\n",
    "                \n",
    "                name =  str(center_ids[sequence]).zfill(6) + '.pcd'\n",
    "                \n",
    "                o3d.io.write_point_cloud(out_dbscan + name,pcd_3duis , write_ascii=False, compressed=False, print_progress=False)\n",
    "                #o3d.io.write_point_cloud(out_kitti + name, kitti_chunk + pcd_chunk_ground, write_ascii=False, compressed=False, print_progress=False)\n",
    "                #o3d.io.write_point_cloud(out_kitti_instance + name, kitti_chunk_instance + ground_chunk, write_ascii=False, compressed=False, print_progress=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_pcds(out_folder_ncuts):\n",
    "        point_clouds = []\n",
    "\n",
    "        # List all files in the folder\n",
    "        files = os.listdir(out_folder_ncuts)\n",
    "        files.sort()\n",
    "\n",
    "        # Filter files with a .pcd extension\n",
    "        pcd_files = [file for file in files if file.endswith(\".pcd\")]\n",
    "        # Load each point cloud and append to the list\n",
    "        for pcd_file in pcd_files:\n",
    "                file_path = os.path.join(out_folder_ncuts, pcd_file)\n",
    "                point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "                point_clouds.append(point_cloud)\n",
    "        return point_clouds\n",
    "\n",
    "def merge_unite_gt(chunks):\n",
    "    last_chunk = chunks[0] \n",
    "    merge = o3d.geometry.PointCloud()\n",
    "    merge += last_chunk\n",
    "\n",
    "    for new_chunk in chunks[1:]:\n",
    "        merge += new_chunk\n",
    "    \n",
    "    merge.remove_duplicated_points()\n",
    "    return merge\n",
    "    \n",
    "\n",
    "def intersect(pred_indices, gt_indices):\n",
    "        intersection = np.intersect1d(pred_indices, gt_indices)\n",
    "        return intersection.size / pred_indices.shape[0]\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_batch(unique_pred, preds, labels, gt_idcs, threshold, new_ncuts_labels):\n",
    "    pred_idcs = np.where(preds == unique_pred)[0]\n",
    "    cur_intersect = np.sum(np.isin(pred_idcs, gt_idcs))\n",
    "    if cur_intersect > threshold * len(pred_idcs): \n",
    "        new_ncuts_labels[pred_idcs] = 0\n",
    "\n",
    "def remove_semantics(labels, preds, threshold=0.8, num_threads=None):\n",
    "    gt_idcs = np.where(labels == 0)[0]\n",
    "    new_ncuts_labels = preds.copy()\n",
    "    unique_preds = np.unique(preds)\n",
    "    \n",
    "    if num_threads is None:\n",
    "        num_threads = min(len(unique_preds), 8)  # Default to 8 threads if not specified\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for i in tqdm(unique_preds):\n",
    "            futures.append(executor.submit(process_batch, i, preds, labels, gt_idcs, threshold, new_ncuts_labels))\n",
    "        \n",
    "        # Wait for all tasks to complete\n",
    "        for future in tqdm(futures, total=len(futures), desc=\"Processing\"):\n",
    "            future.result()  # Get the result to catch any exceptions\n",
    "        \n",
    "    return new_ncuts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dbscan =  out_chunks +  'out_3duis' +  str(SEQUENCE_NUM) + '/'\n",
    "point_clouds_dbscan = get_merge_pcds(out_dbscan)[:-1]\n",
    "#point_clouds_kitti = get_merge_pcds(out_kitti)[:-1]\n",
    "#merge_kitti = merge_unite_gt(point_clouds_kitti)\n",
    "#o3d.io.write_point_cloud(out_folder + 'kitti_labels.pcd',merge_kitti_instance)\n",
    "\n",
    "\n",
    "merge_dbscan = merge_chunks_unite_instances(point_clouds_dbscan)\n",
    "o3d.io.write_point_cloud(out_folder + '3duis_' + str(SEQUENCE_NUM)  + '.pcd',merge_dbscan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nunique_colors, labels_dbscan = np.unique(np.asarray(merge_dbscan.colors), axis=0, return_inverse=True)\\nunique_colors, labels_kitti = np.unique(np.asarray(merge_kitti_instance.colors),axis=0, return_inverse=True)\\n\\ndef intersect(pred_indices, gt_indices):\\n        intersection = np.intersect1d(pred_indices, gt_indices)\\n        return intersection.size / pred_indices.shape[0]\\n\\n\\ndef remove_semantics(labels,preds):\\n        gt_idcs = np.where(labels == 0)[0]\\n        new_ncuts_labels = preds.copy()\\n        for i in np.unique(preds):\\n                pred_idcs = np.where(preds == i)[0]\\n                cur_intersect = intersect(pred_idcs,gt_idcs)\\n                if cur_intersect > 0.8:\\n                        new_ncuts_labels[pred_idcs] = 0\\n        return new_ncuts_labels\\n\\nnew_dbscan_labels = remove_semantics(labels_kitti,labels_dbscan)\\n\\n\\nmetrics_dbscan = Metrics(name='dbscan')\\nmetrics_test = Metrics(name='test')\\n\\n\\nmetrics_dbscan.update_stats(new_dbscan_labels,labels_kitti)\\n#metrics_dbscan.compute_all_aps()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "unique_colors, labels_dbscan = np.unique(np.asarray(merge_dbscan.colors), axis=0, return_inverse=True)\n",
    "unique_colors, labels_kitti = np.unique(np.asarray(merge_kitti_instance.colors),axis=0, return_inverse=True)\n",
    "\n",
    "def intersect(pred_indices, gt_indices):\n",
    "        intersection = np.intersect1d(pred_indices, gt_indices)\n",
    "        return intersection.size / pred_indices.shape[0]\n",
    "\n",
    "\n",
    "def remove_semantics(labels,preds):\n",
    "        gt_idcs = np.where(labels == 0)[0]\n",
    "        new_ncuts_labels = preds.copy()\n",
    "        for i in np.unique(preds):\n",
    "                pred_idcs = np.where(preds == i)[0]\n",
    "                cur_intersect = intersect(pred_idcs,gt_idcs)\n",
    "                if cur_intersect > 0.8:\n",
    "                        new_ncuts_labels[pred_idcs] = 0\n",
    "        return new_ncuts_labels\n",
    "\n",
    "new_dbscan_labels = remove_semantics(labels_kitti,labels_dbscan)\n",
    "\n",
    "\n",
    "metrics_dbscan = Metrics(name='dbscan')\n",
    "metrics_test = Metrics(name='test')\n",
    "\n",
    "\n",
    "metrics_dbscan.update_stats(new_dbscan_labels,labels_kitti)\n",
    "#metrics_dbscan.compute_all_aps()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 46671.44it/s]\n",
      "Processing: 100%|██████████| 160/160 [00:14<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for file dbscan\n",
      "{'panoptic': 0.7990783663474457, 'precision': 1.0, 'recall': 0.8, 'fScore': 0.888888888888889, 'usr': 0.0, 'osr': 0.0, 'noise': 0.0, 'missed': 0.2, 'mean': 0.8989631621408763}\n",
      "lstq value :  0.7076576323143998\n",
      "Average Precision @ 0.25 0.8\n",
      "Average Precision @ 0.5 0.8\n",
      "Average Precision @ 0.55 0.8\n",
      "Average Precision @ 0.6 0.8\n",
      "Average Precision @ 0.65 0.714124579124579\n",
      "Average Precision @ 0.7 0.714124579124579\n",
      "Average Precision @ 0.75 0.714124579124579\n",
      "Average Precision @ 0.8 0.5623364598364597\n",
      "Average Precision @ 0.85 0.3605483405483405\n",
      "Average Precision @ 0.9 0.3028210678210678\n",
      "Average Precision @ 0.95 0.12257575757575756\n",
      "AP @ 0.25 80.0\n",
      "AP @ 0.5 80.0\n",
      "AP @ [0.5:0.95] 58.907\n"
     ]
    }
   ],
   "source": [
    "#merge_dbscan_instances = color_pcd_by_labels(merge_dbscan,new_dbscan_labels)\n",
    "merge_dbscan = o3d.io.read_point_cloud(out_folder + '3duis_' + str(SEQUENCE_NUM)  + '.pcd')\n",
    "merge_kitti_instance = o3d.io.read_point_cloud(out_folder + 'merge_part_kitti_instance' + str(SEQUENCE_NUM)+ '.pcd')\n",
    "#o3d.visualization.draw_geometries([merge_dbscan])\n",
    "#o3d.visualization.draw_geometries([merge_kitti_instance])\n",
    "\n",
    "unique_colors, labels_dbscan = np.unique(np.asarray(merge_dbscan.colors), axis=0, return_inverse=True)\n",
    "unique_colors, labels_kitti = np.unique(np.asarray(merge_kitti_instance.colors),axis=0, return_inverse=True)\n",
    "\n",
    "new_3duis_labels = remove_semantics(labels_kitti,labels_dbscan)\n",
    "metrics_dbscan = Metrics(name='dbscan')\n",
    "\n",
    "metrics_dbscan.update_stats(labels_dbscan,new_3duis_labels,labels_kitti,calc_all=True)\n",
    "\n",
    "\n",
    "#o3d.visualization.draw_geometries([merge_dbscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dbscan = o3d.io.read_point_cloud('pcd_preprocessed/3duis_7.pcd')\n",
    "o3d.visualization.draw_geometries([merge_dbscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Metrics' object has no attribute 'recall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cedric/unsup_3d_instances/point-to-pixel-mapping/3duis_processing.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cedric/unsup_3d_instances/point-to-pixel-mapping/3duis_processing.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cedric/unsup_3d_instances/point-to-pixel-mapping/3duis_processing.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(metrics_dbscan\u001b[39m.\u001b[39;49mrecall,metrics_dbscan\u001b[39m.\u001b[39mprecision)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cedric/unsup_3d_instances/point-to-pixel-mapping/3duis_processing.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Metrics' object has no attribute 'recall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(metrics_dbscan.recall,metrics_dbscan.precision)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
